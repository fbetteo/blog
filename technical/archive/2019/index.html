
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../2020/">
      
      
        <link rel="next" href="../2018/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.2.7">
    
    
      
        <title>2019 - Franco Betteo</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.046329b4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.85d0ee34.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="youtube" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#2019" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Franco Betteo" class="md-header__button md-logo" aria-label="Franco Betteo" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Franco Betteo
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2019
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="youtube" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../.." class="md-tabs__link">
          
  
    
  
  Home

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
    
  
  Technical posts

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="https://sportsjobs.online" class="md-tabs__link">
        
  
    
  
  Job Board

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../nba_salaries/" class="md-tabs__link">
          
  
    
  
  NBA salaries legacy model

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Franco Betteo" class="md-nav__button md-logo" aria-label="Franco Betteo" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Franco Betteo
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../.." class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Home
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
    
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Technical posts
  </span>
  

            </a>
            
              <label class="md-nav__link " for="__nav_2">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Technical posts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" checked>
        
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Archive
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../2024/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../2022/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../2021/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../2020/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    2019
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    2019
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#distintas-distancias" class="md-nav__link">
    Distintas Distancias
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#maxima-verosimilitud-y-estimacion-bayesiana" class="md-nav__link">
    Maxima Verosimilitud y estimacion bayesiana
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#teorema-central-del-limite" class="md-nav__link">
    Teorema Central del Limite
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metodos-de-resampleo-islr-capitulo-5" class="md-nav__link">
    Metodos de Resampleo - ISLR Capitulo 5
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regresion-lineal-islr-capitulo-3" class="md-nav__link">
    Regresion Lineal - ISLR Capítulo 3
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#aprendizaje-estadistico-islr-capitulo-2" class="md-nav__link">
    Aprendizaje Estadístico - ISLR Capitulo 2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#esencia-del-algebra-lineal" class="md-nav__link">
    Esencia del Algebra Lineal
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#funciones-de-probabilidad-y-distribucion" class="md-nav__link">
    Funciones de Probabilidad y Distribucion
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduccion-a-graficos-con-mapas" class="md-nav__link">
    Introduccion a graficos con mapas
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../2018/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Categories
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Categories
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../category/estadistica/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    estadistica
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../category/machine-learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    machine learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../category/statistics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    statistics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../category/python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../category/r/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    R
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../category/algebra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    algebra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../category/matematica/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    matematica
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../category/blog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blog
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="https://sportsjobs.online" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Job Board
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../nba_salaries/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    NBA salaries legacy model
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#distintas-distancias" class="md-nav__link">
    Distintas Distancias
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#maxima-verosimilitud-y-estimacion-bayesiana" class="md-nav__link">
    Maxima Verosimilitud y estimacion bayesiana
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#teorema-central-del-limite" class="md-nav__link">
    Teorema Central del Limite
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metodos-de-resampleo-islr-capitulo-5" class="md-nav__link">
    Metodos de Resampleo - ISLR Capitulo 5
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regresion-lineal-islr-capitulo-3" class="md-nav__link">
    Regresion Lineal - ISLR Capítulo 3
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#aprendizaje-estadistico-islr-capitulo-2" class="md-nav__link">
    Aprendizaje Estadístico - ISLR Capitulo 2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#esencia-del-algebra-lineal" class="md-nav__link">
    Esencia del Algebra Lineal
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#funciones-de-probabilidad-y-distribucion" class="md-nav__link">
    Funciones de Probabilidad y Distribucion
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduccion-a-graficos-con-mapas" class="md-nav__link">
    Introduccion a graficos con mapas
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content" data-md-component="content">
    <div class="md-content__inner">
      <header class="md-typeset">
        <h1 id="2019">2019</h1>
      </header>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2019-11-11 00:00:00">2019-11-11</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../../category/algebra/" class="md-meta__link">algebra</a>, 
              <a href="../../category/estadistica/" class="md-meta__link">estadistica</a>, 
              <a href="../../category/matematica/" class="md-meta__link">matematica</a></li>
        
        
          
          <li class="md-meta__item">
            
              7 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="distintas-distancias"><a class="toclink" href="../../2019/11/11/distintas-distancias/">Distintas Distancias</a></h2>
<pre><code class="language-r">library(tidyverse)
</code></pre>
<p>Si tenemos un espacio euclideo, es decir, una linea, un plano o un hiperplano, que son los espacios
típicos de la geometría clásica, podemos calcular la distancia entre dos puntos que se hayen en él.</p>
<p>Es decir, cuál es la distancia entre los puntos A (1,1) y B (1,0) en un plano?
Empecemos pensando en los casos donde todos los valores del vector son numéricos.</p>
<pre><code class="language-r">A = c(0,0,1,1)
B = c(0,0,1,0)
recta = c(1,1,1,0)
df = as.data.frame(matrix(data = c(A,B, recta),
                          nrow = 4, 
                          ncol = 4,
                          byrow = TRUE )) %&gt;%
  rename( x0 = V1,
          y0 = V2,
          x1 = V3,
          y1 = V4)


ggplot(data=df[1:2,], aes(x=x0, y=y0)) + 
  geom_segment(aes(xend=x1, yend=y1),
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + 
  geom_point( aes(x = x1, y = y1), 
              color = &quot;red&quot;, size = 2)
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-2-1.png" /></p>
<h4 id="distancia-euclideana"><a class="toclink" href="../../2019/11/11/distintas-distancias/#distancia-euclideana">Distancia Euclideana</a></h4>
<p>La métrica más habitual que se utiliza es la distancia euclideana, que consiste en la recta que une ambos puntos. </p>
<pre><code class="language-r">ggplot(data=df[1:2,], aes(x=x0, y=y0))+
  geom_segment(aes(xend=x1, yend=y1),
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + 
  geom_point(aes(x = x1, y = y1), 
               color = &quot;red&quot;, size = 2) + 
  geom_segment(data = df[3,], 
               aes(xend=x1, yend=y1),
               color = &quot;blue&quot;, 
               arrow = arrow(length = unit(0.3,&quot;cm&quot;)))
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-3-1.png" /></p>
<p>Esta distancia se calcula con:<br />
<span class="arithmatex">\(<span class="arithmatex">\(d(A,B) = d(B,A) = \sqrt{(A_1 - B_1)^2 + (A_2 - B_2)^2 + ... + (A_n - B_n)^2}  
= \sqrt{\sum_{i=1}^n (A_i - B_i)^2}\)</span>\)</span></p>
<p>Como se ve en la imagen, los puntos A y B pueden verse como vectores que inician en el origen (0,0). La distancia euclidea es a su vez la distancia entre sus puntas, que a su vez puede pensarse como un vector de desplazamiento (de A a B por ejemplo).</p>
<p>En este caso la distancia euclidea es:
<span class="arithmatex">\(<span class="arithmatex">\(d(A,B) = \sqrt{ (1-1)^2 + (1 - 0)^2} = 1\)</span>\)</span>
Y que es algo visible en el gráfico.</p>
<p>De manera más general, podemos definir toda una familia de distancias en el espacio euclideo.
<em>Las distancias de Minkowsky.</em></p>
<p>La distancia Minkowsky de orden p es:
<span class="arithmatex">\(<span class="arithmatex">\(d(A,B) = d(B,A) = \Bigg({\sum_{i=1}^n |A_i - B_i|^p}\Bigg)^{1/p}\)</span>\)</span></p>
<p>Vemos que si p = 2, entonces la distancia de Minkowsky no es otra que la distancia euclideana.</p>
<h4 id="distancia-de-manhattan"><a class="toclink" href="../../2019/11/11/distintas-distancias/#distancia-de-manhattan">Distancia de Manhattan</a></h4>
<p>Otro valor que suele tomarse para p es p = 1, y eso corresponde a la <em>distancia de Manhattan</em>.</p>
<p>Esta distancia se calcula con:<br />
<span class="arithmatex">\(<span class="arithmatex">\(d(A,B) = d(B,A) =  |A_1 - B_1| + |A_2 - B_2| + ... + |A_n - B_n|  
=\sum_{i=1}^n |A_i - B_i|\)</span>\)</span></p>
<p>Es básicamente la suma de las diferencias absolutas entre las distintas dimensiones de los vectores.</p>
<p>Luce asi.</p>
<pre><code class="language-r">A = c(0,0,3,3)
B = c(0,0,2,1)
recta = c(2,1,3,3)
manhattan1 = c(2,1,3,1)
manhattan2 = c(3,1,3,3)

df = as.data.frame(matrix(data = c(A,B, recta, manhattan1, manhattan2),
                          nrow = 5, 
                          ncol = 4,
                          byrow = TRUE )) %&gt;%
  rename( x0 = V1,
          y0 = V2,
          x1 = V3,
          y1 = V4)


ggplot(data=df[1:2,], aes(x=x0, y=y0)) + 
    geom_point( aes(x = x1, y = y1), 
              color = &quot;red&quot;, size = 2) + 
      geom_segment(data = df[3,], 
               aes(xend=x1, yend=y1, color = &quot;blue&quot;),
               #color = &quot;blue&quot;, 
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) +
      geom_segment(data = df[4,], 
               aes(xend=x1, yend=y1,  color = &quot;green&quot;,),
               #color = &quot;green&quot;, 
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + 
      geom_segment(data = df[5,], 
               aes(xend=x1, yend=y1),
               color = &quot;green&quot;, 
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) +
      scale_colour_manual(name = 'the colour', 
         values =c('blue'='blue','green'='green'),
         labels = c('Euclideana','Manhattan'))
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-4-1.png" /></p>
<p>Vemos como el valor abosluto imposibilita ir en dirección diagonal. Lo que se logra es medir la distancia como si hubiera una grilla como la del gráfico. Su nombre proviene de su utilización para medir distancias al interior de una ciudad (uno no puede cruzar las manzanas por el medio!).</p>
<p>Para saber cual conviene utilizar hay que pensar en el problema en cuestión. </p>
<ul>
<li>Ya sea medir distancias en ciudades o donde haya restricciones de ese tipo puede que Manhattan sea más apropiado.  </li>
<li>Por otra parte al no elevar al cuadrado le da menos pesos a las grandes distancias o mismo outliers por lo que puede ser otro motivo válido.  </li>
<li>Por último, algunos trabajos argumentan que es más adecuada en problema de alta dimensionalidad (o mismo valores menores a 1 en el exponente de la formula de Minkowsky)</li>
</ul>
<h4 id="similaridad-coseno"><a class="toclink" href="../../2019/11/11/distintas-distancias/#similaridad-coseno">Similaridad coseno</a></h4>
<p>La similaridad coseno se utiliza cuando se quiere ver la similitud "angular" entre dos observaciones y no la distancia en el plano. Es decir, vemos la dirección pero no la magnitud</p>
<pre><code class="language-r">A = c(0,0,1,1)
B = c(0,0,2,2)
C = c(0,0,5,0)

df = as.data.frame(matrix(data = c(A,B,C),
                          nrow = 3, 
                          ncol = 4,
                          byrow = TRUE )) %&gt;%
  rename( x0 = V1,
          y0 = V2,
          x1 = V3,
          y1 = V4)


ggplot(data=df[1:3,], aes(x=x0, y=y0 )) + 
  geom_segment(aes(xend=x1, yend=y1),
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + 
  geom_point( aes(x = x1, y = y1), 
              color = &quot;red&quot;, size = 2) + 
  geom_text(aes(x=x1, y = y1, label = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;)),
            vjust = -0.5)
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-5-1.png" /></p>
<p>Si hicieramos la distancia euclideando entre A y B obtendriamos el valor de la distancia en el plano, sin embargo podemos ver que se encuentran sobre la misma recta y por lo tanto su dirección es la misma. La similaridad coseno mide el ángulo entre dos puntos. En este caso el ángulo entre A y B es 0, y por ende su similaridad coseno es 1. Ambas tendrían la misma similaridad con cualquier otro punto de la misma recta, por más alejado que esté.
Respecto a C, tanto A y B tiene comparten el ángulo por lo tanto la similaridad coseno entre A y C será la misma que entre B y C.</p>
<pre><code class="language-r">cosA = c(1,1)
cosB = c(2,2)
cosC = c(5,0)

# Similaridad coseno entre A y B
lsa::cosine(cosA, cosB)[[1]]
</code></pre>
<pre><code>## [1] 1
</code></pre>
<pre><code class="language-r"># Similaridad coseno entre A y C
lsa::cosine(cosA, cosC)[[1]]
</code></pre>
<pre><code>## [1] 0.7071068
</code></pre>
<pre><code class="language-r"># Similaridad coseno entre B y C
lsa::cosine(cosC, cosB)[[1]]
</code></pre>
<pre><code>## [1] 0.7071068
</code></pre>
<p>Hay que tener en cuenta el contexto de nuestro problema para decidir qué medida de distancia usar. Por ejemplo la similaridad coseno se usa de manera estándar en análisis de texto (text mining).</p>
<h4 id="distancia-de-mahalanobis"><a class="toclink" href="../../2019/11/11/distintas-distancias/#distancia-de-mahalanobis">Distancia de Mahalanobis</a></h4>
<p>La distancia de Mahalanobis tiene la particularidad que mide la distancia entre un punto (P) y una distribución de datos (D). Si tenemos una nube de puntos correspondiente a una distribución D, cuanto más cerca esté P del centro de masa (o "promedio") más cerca se encuetran P y D. Intuitivamente sirve para pensar si P puede pertenecer a D o no.<br />
Dado que la nube de puntos no tiene por qué ser una esfera (donde cada dirección tiene la misma cantidad de puntos), hay que tener en cuenta cómo se dispersan los puntos alrededor del centro de masa.</p>
<p>No es lo mismo, </p>
<pre><code class="language-r">esfera = as.data.frame(MASS::mvrnorm(1000, mu = c(3,3), 
                                     Sigma = matrix(c(1,0,0,1),
                                                    nrow = 2,
                                                    ncol = 2)))

ggplot(data = esfera, aes(x = V1, y = V2)) + 
  geom_point() + 
  geom_point(data = as.data.frame(matrix(c(6,2),ncol = 2)), 
             aes(x = V1, y = V2), color = &quot;red&quot;) + 
  geom_text(data = as.data.frame(matrix(c(6,2),ncol = 2)),
            aes(x = V1, y = V2,label = &quot;P&quot;),
            vjust = 1.5, color = &quot;blue&quot;) +
  labs(title = &quot;Distribución esférica&quot;)
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-7-1.png" /></p>
<p>que,</p>
<pre><code class="language-r">elipse = as.data.frame(MASS::mvrnorm(1000, mu = c(3,3), 
                                     Sigma = matrix(c(1,0.6,0.6,1),
                                                    nrow = 2,
                                                    ncol = 2)))



ggplot(data = elipse, aes(x = V1, y = V2)) + 
  geom_point() + 
  geom_point(data = as.data.frame(matrix(c(6,2),ncol = 2)), 
             aes(x = V1, y = V2), color = &quot;red&quot;) + 
  geom_text(data = as.data.frame(matrix(c(6,2),ncol = 2)),
            aes(x = V1, y = V2,label = &quot;P&quot;),
            vjust = 1.5, color = &quot;blue&quot;) +
  labs(title = &quot;Distribución Elíptica&quot;)
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-8-1.png" /></p>
<p>Los centros de masa son los mismos y lo único que cambia es la matriz de variancias y covarianzas (o como se correlacionan las variables). La distancia de P al centro es la misma, pero está claro que en el caso esférico P se encuentra más cerca de la distribución que en el caso elíptico.</p>
<p>Mahalanobis tiene en cuenta este aspecto ya que involucra la matriz de varianzas y covarianzas.</p>
<p>La distancia entre el punto x y la distribución con vector de medias <span class="arithmatex">\(\vec{\mu}\)</span> y matriz de covarianzas S es:
$$ D_M(\vec{x}) = \sqrt{(\vec{x} - \vec{\mu})^TS^{-1}(\vec{x} - \vec{\mu})})$$</p>
<p>Tanto el vector <span class="arithmatex">\(\vec{x}\)</span> como la distribución pueden ser multivariadas (como se ve en los gráficos de arriba).</p>
<p>Tener en cuenta que si tenemos dos puntos provenientes de la misma distribución, podemos usar la distancia de Mahalanobis como una medida de disimilaridad:
$$ D_M(\vec{x},\vec{y}) = \sqrt{(\vec{x} - \vec{y})^TS^{-1}(\vec{x} - \vec{y})})$$
Veamos por ejemplo como queda la distancia de P a las distribuciones esféricas y elípticas graficadas.</p>
<pre><code class="language-r"># Caso Esférico

mahalanobis(x = c(6,2), 
            center = c(3,3), 
            cov = matrix(c(1,0,0,1),
                         nrow = 2,
                         ncol = 2))
</code></pre>
<pre><code>## [1] 10
</code></pre>
<pre><code class="language-r"># Caso Elíptico
mahalanobis(x = c(6,2), 
            center = c(3,3), 
            cov = matrix(c(1,0.6,0.6,1),
                         nrow = 2,
                         ncol = 2))
</code></pre>
<pre><code>## [1] 21.25
</code></pre>
<p>Queda claro que P es más cercano a la distribución esférica que a la elíptica.</p>

    <nav class="md-post__action">
      <a href="../../2019/11/11/distintas-distancias/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2019-10-31 00:00:00">2019-10-31</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../../category/estadistica/" class="md-meta__link">estadistica</a>, 
              <a href="../../category/r/" class="md-meta__link">R</a></li>
        
        
          
          <li class="md-meta__item">
            
              7 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="maxima-verosimilitud-y-estimacion-bayesiana"><a class="toclink" href="../../2019/10/31/maxima-verosimilitud-y-estimacion-bayesiana/">Maxima Verosimilitud y estimacion bayesiana</a></h2>
<h3 id="distribucion-prior"><a class="toclink" href="../../2019/10/31/maxima-verosimilitud-y-estimacion-bayesiana/#distribucion-prior">Distribucion prior</a></h3>
<p>A falta de una buena traducción usamos este término.</p>
<p>Supongamos que se toman muestras aleatorias de una distribucion con <a href="https://fbetteo.netlify.com/2019/04/funciones-de-probabilidad-y-distribucion/">pdf (funcion de densidad de probabilidad)</a> <span class="arithmatex">\(f(x|\theta)\)</span>. Por ejemplo podrían provenir de una normal con media = <span class="arithmatex">\(\mu\)</span> y varianza = 4.<br />
Nosotros no sabemos el valor de <span class="arithmatex">\(\mu\)</span> pero podemos tener una idea de qué valores puede tomar y tener en mente una distribución prior de este parámetro <span class="arithmatex">\(\epsilon(\theta)\)</span>. Para el ejemplo sería <span class="arithmatex">\(\epsilon(\mu)\)</span>. Podemos suponer que <span class="arithmatex">\(\mu\)</span> se distribuye como una uniforme (0,1) por decir algo.<br />
El concepto radica en tener una distribución prior para los parámetros de la distribución de la cual tomamos muestras aleatorias.</p>
<h3 id="distribucion-posterior"><a class="toclink" href="../../2019/10/31/maxima-verosimilitud-y-estimacion-bayesiana/#distribucion-posterior">Distribución Posterior</a></h3>
<p>Volviendo a nuestra muestra <span class="arithmatex">\(X_1...X_n\)</span> proveniente de <span class="arithmatex">\(f(x|\theta)\)</span>, podemos decir, debido a que son observaciones aleatorias e independientes que su distribución conjunta es <span class="arithmatex">\(f_n(x_1...X_n|\theta) = f(x_1|\theta)...f(x_n|\theta)\)</span>, que lo podemos escribir como <span class="arithmatex">\(f_n(x|\theta)\)</span>.<br />
Dado que suponemos que <span class="arithmatex">\(\theta\)</span> proviene de una distribución <span class="arithmatex">\(\epsilon(\theta)\)</span>, la pdf conjunta <span class="arithmatex">\(f_n(x|\theta)\)</span> tiene que ser vista como la pdf conjunta condicional de<span class="arithmatex">\(X_1...X_n\)</span> para un valor particular de <span class="arithmatex">\(\theta\)</span>.<br />
Multiplicando la pdf conjunta condicional por la pdf <span class="arithmatex">\(\epsilon(\theta)\)</span> obtenemos la (n+1) distribución conjunta de <span class="arithmatex">\(X_1...X_n\)</span> y <span class="arithmatex">\(\theta\)</span> bajo la forma <span class="arithmatex">\(f_n(x|\theta)\epsilon(\theta)\)</span>. Sería la pdf de encontrar en simultáneo determinados valores para x y <span class="arithmatex">\(\theta\)</span>. La probabilidad conjunta marginal de <span class="arithmatex">\(X_1...X_n\)</span> se encuentra integrando la pdf conjunta con <span class="arithmatex">\(\theta\)</span> para todos los valores de <span class="arithmatex">\(\theta\)</span>. Sería la probabilidad marginal de encontrar determinados valores de x (sabiendo la distribución de <span class="arithmatex">\(\theta\)</span> pero sin saber el valor puntual que toma).</p>
<p><span class="arithmatex">\(g_n(x) = \int_\Omega f_n(x|\theta)\epsilon(\theta) d\theta\)</span></p>
<p>Por teorema de Bayes tenemos que la distribución posterior de <span class="arithmatex">\(\theta\)</span>, es decir, dados los x es:
<span class="arithmatex">\(<span class="arithmatex">\(\epsilon(\theta|x) = \frac{f_n(x|\theta)\epsilon(\theta)}{g_n(x)} \text{ para } \theta \in \Omega\)</span>\)</span> 
 Se dice que la distribución prior <span class="arithmatex">\(\epsilon(\theta)\)</span> representa la verosimilitud, antes de ver los valores de <span class="arithmatex">\(X_1...X_n\)</span>, de que el verdadero valor de <span class="arithmatex">\(\theta\)</span> se encuentre en cada una de las regiones de <span class="arithmatex">\(\Omega\)</span> y que la pdf de la distribución posterior <span class="arithmatex">\(\epsilon(\theta|x)\)</span> representa la verosimilitud después que los valores <span class="arithmatex">\(X_1 = x_1,...,X_n = x_n\)</span> hayan sido observados.</p>
<p>## La funcion de Versoimilitud</p>
<p>El denominador de la distribución posterior es básicamente la integral del numerador para todos los posibles valores de <span class="arithmatex">\(\theta\)</span>. Depende de los valores observados <span class="arithmatex">\(X_1...X_n\)</span> pero no de <span class="arithmatex">\(\theta\)</span>, por lo que puede considerarse constante en este contexto.<br />
 Dado que es una constante podemos quitarla de la distribución posterior que vimos y decir que 
 <span class="arithmatex">\(<span class="arithmatex">\(\epsilon(\theta|x) \propto f_n(x|\theta)\epsilon(\theta)\)</span>\)</span></p>
<p>Cuando se ve <span class="arithmatex">\(f_n(x|\theta)\)</span> para una muestra aleatoria como función de <span class="arithmatex">\(\theta\)</span>, se la suele llamar función de verosimilitud. En inglés: Likelihood function.</p>
<p>Juntando estos términos podemos decir que la pdf posterior de <span class="arithmatex">\(\theta\)</span> es proporcional al producto de la función de verosimilitud y la pdf prior de <span class="arithmatex">\(\theta\)</span>.  </p>
<p>La idea de ver esta relación de proporcionalidad es para poder calcular la pdf posterior evitando calcular la integral del denomiador <span class="arithmatex">\(g_n(x)\)</span>. Si el numerador tiene la forma de alguna de las distribuciones conocidad (normal, beta, gamma, uniforme, etc) es posible calcular fácilmente el factor constante por el cual multiplicar esa pdf para llegar a la posterior.</p>
<h3 id="distribuciones-prior-conjugadas"><a class="toclink" href="../../2019/10/31/maxima-verosimilitud-y-estimacion-bayesiana/#distribuciones-prior-conjugadas">Distribuciones prior Conjugadas</a></h3>
<p>Este concepto refiere a que ciertas distribuciones son particularmente útiles para los cálculos cuando las variables aleatorias observadas provienen de alguna distribución específica.<br />
Es decir que según la distribución de la que provienen las X puede que haya alguna distribución conjugada tal que al asumirla para la pdf prior <span class="arithmatex">\(\epsilon(\theta)\)</span> ya sabemos que la distribución posterior también será de esa familia.</p>
<p>Un ejemplo ilustrador:<br />
  Supongamos que tomamos observaciones <span class="arithmatex">\(X_1...X_n\)</span> de una distribución Bernoulli de la cual no sabemos el parámetro <span class="arithmatex">\(\theta\)</span> (que debe estar entre 0 y 1). Supongamos además que la pdf prior de <span class="arithmatex">\(\theta\)</span> es una distribución beta con algúnos parámetros dados <span class="arithmatex">\(\alpha \text{ y } \beta\)</span>. En este caso sabemos que por ser un caso de distribución conjugada, la pdf posterior de <span class="arithmatex">\(\theta\)</span> dado <span class="arithmatex">\(X = x_i (i = 1,...,n)\)</span> es a su vez una distribución beta con parámetros <span class="arithmatex">\(\alpha + \sum_{i=1}^n x_i \text{ y } \beta + n - \sum_{i=1}^n x_i\)</span>.</p>
<p>Según la distribución de la que provengan las observaciones hay distintas distribuciones conjugadas que son las más convenientes para ese caso.</p>
<h2 id="estimacion-de-parametros"><a class="toclink" href="../../2019/10/31/maxima-verosimilitud-y-estimacion-bayesiana/#estimacion-de-parametros">Estimación de parámetros</a></h2>
<p>La idea es estimar algún parámetro de la distribución de la cual se obtienen los datos observados. El valor estimado del parámetro va a depender de dos cosas:  </p>
<ul>
<li>Del <em>estimador</em> que hayamos elegido (es decir, la función de los datos elegida)</li>
<li>De la muestra. El valor estimado va a depender de los datos aleatorios que tengamos de la distribución.</li>
</ul>
<p>Como el estimador depende de la muestra podemos verlo a su vez como una variable aleatoria.</p>
<h3 id="funcion-de-perdida"><a class="toclink" href="../../2019/10/31/maxima-verosimilitud-y-estimacion-bayesiana/#funcion-de-perdida">Función de pérdida</a></h3>
<p>Lo que queremos de un estimador es que devuelva un valor estimado "a" para el parámetro lo más cercano posible al verdadero valor de <span class="arithmatex">\(\theta\)</span>. La función de pérdida es una función que cuantifica esto.
$$ L(\theta,a)$$
Hay algunas funciones habituales pero pueden adecuarse según el problema.<br />
Podemos decir que en general lo que se busca es encontrar una estimación para la cual la esperanza de la pérdida sea un mínimo.</p>
<h3 id="estimador-bayesiano"><a class="toclink" href="../../2019/10/31/maxima-verosimilitud-y-estimacion-bayesiana/#estimador-bayesiano">Estimador bayesiano</a></h3>
<p>Si tenemos una muestra aleatoria y una pdf posterior para <span class="arithmatex">\(\theta\)</span> entonces el valor esperado de la pérdida para cualquier valor estimado "a" es:
<span class="arithmatex">\(<span class="arithmatex">\(E[L(\theta,a)|x] = \int_\Omega L(\theta,a)\epsilon(\theta,x)d\theta\)</span>\)</span></p>
<p>Lo que buscamos es encontrar un valor de a cuya pérdida esperada sea mínima. La función que genera un valor de a mínimo para cada posible valor de X será un estimador de <span class="arithmatex">\(\theta\)</span> y en particular se llamará <em>Estimador Bayesiano</em>.<br />
El estimador bayesiano, que minimiza la pérdida esperada para cualquier set de datos X, va a depender de la función de pérdida que elijamos y de la pdf prior que se elija para <span class="arithmatex">\(\theta\)</span>.</p>
<p>Por ejemplo,para la función de pérdida más utilizada, que es la de error cuadrático
<span class="arithmatex">\(<span class="arithmatex">\(L(\theta,a) = (\theta -a)^2\)</span>\)</span>
está demostrado que la pérdida es mínima cuando <span class="arithmatex">\(a\)</span> es la media de la distribución posterior <span class="arithmatex">\(E(\theta|x)\)</span>.</p>
<p>Dijimos que el valor del estimador bayesiano va a depender de la distribución prior elegida. Esto es cierto, pero hay que tener en cuenta que para muestras grandes las diferencias empiezan a achicarse y los estimadores bayesianos provenientes de distintos priors empiezan a converger en la mayoría de los casos.</p>
<h3 id="estimadores-de-maxima-verosimilitud"><a class="toclink" href="../../2019/10/31/maxima-verosimilitud-y-estimacion-bayesiana/#estimadores-de-maxima-verosimilitud">Estimadores de Máxima Verosimilitud</a></h3>
<p>Los estimadores de máxima verosimilitud (MLE) son muy comunmente usados para estimar parámetros desconocidos ya que más allá de la discusión casi filosófica de "bayesianos vs frecuentistas", sirven para estimar sin tener que definir una función de pérdida ni una distribución prior para los parámetros. Esto último es importante ya que para casos donde se necesita estimar un vector de parámetros, la distribución prior debe ser una multivariada que englobe a todos y eleva la complejidad del proceso bayesiano ampliamente.<br />
Para muestras chicas MLE suele hacer un trabajo decente y para muestras grandes suele ser excelente por lo que se llega a resultados muy similares a través de un proceso más directo y más sencillo.  </p>
<p>Para estimar mediante MLE lo único que necesitamos es la función de verosimilitud ya definida.
<span class="arithmatex">\(<span class="arithmatex">\(f_n(x_1...X_n|\theta)\)</span>\)</span>
Luego lo único que se hace es buscar el parámetro <span class="arithmatex">\(\hat \theta\)</span> (estimado) que maximice esa función. Básicamente es buscar qué parámetro hace que la probabilidad conjunta de obtener esos valores de X sea máxima? Ese es nuestro MLE.</p>
<p>Para la gran mayoría de los casos esta metodología funciona pero hay que tener en cuenta que es posible que para algunos problemas no haya un máximo para la función de verosimilitud o que haya más de un punto, en cuyo caso hay que elegir alguno de ellos.</p>
<h4 id="mle-en-bernoulli"><a class="toclink" href="../../2019/10/31/maxima-verosimilitud-y-estimacion-bayesiana/#mle-en-bernoulli">MLE en Bernoulli</a></h4>
<p>Supongamos que tomamos observaciones <span class="arithmatex">\(X_1...X_n\)</span> de una distribución Bernoulli de la cual no sabemos el parámetro <span class="arithmatex">\(\theta\)</span> (que debe estar entre 0 y 1).</p>
<p>Para cualquier vector de observaciones <span class="arithmatex">\(X_1...X_n\)</span> la función de verosimilitud es:
$$ f_n(x|\theta) = \prod_{i = 1}^n \theta^{x_i}(1-\theta)^{1-x_i}$$
El valor de <span class="arithmatex">\(\theta\)</span> que maximice la función de verosimilitud es el mismo valor que maximiza <span class="arithmatex">\(log f_n(x|\theta)\)</span>, por lo que es conveniente encontrar tal valor buscando que maximice:
<span class="arithmatex">\(<span class="arithmatex">\(L(\theta) = log f_n(x|\theta) = \sum_{i=1}^n[x_i log \theta + (1 - x_i) log(1-\theta)] = (\sum_{i=1}^nx_i)log \theta + (n-\sum_{i=1}^n x_i) log (1-\theta)\)</span>\)</span></p>
<p>Si derivamos <span class="arithmatex">\(dL(\theta) / d\theta\)</span> e igualamos a 0, resolviendo esa ecuando para <span class="arithmatex">\(\theta\)</span> encontramos que <span class="arithmatex">\(\theta = \bar x_n\)</span>.<br />
Este valor maximiza el logaritmo de la función de verosimilitud y por ende también de la función de verosimilitud en sí misma. Por lo tanto el MLE de <span class="arithmatex">\(\theta\)</span> es <span class="arithmatex">\(\hat \theta = \bar X_n\)</span></p>
<pre><code class="language-r"># Generamos 100 observaciones de una Bernoulli
set.seed(150)
data = rbinom(100, 1, prob = 0.723)

# Calculamos su promedio, que ya sabemos es la mejor estimación para p dados los datos
mean(data)
</code></pre>
<pre><code>## [1] 0.68
</code></pre>
<pre><code class="language-r"># Definimos función de verosimilitud
# Es la pdf de una Bernoulli para cada observación y sumamos sus logaritmos (en negativo porque 
# el optimizador minimiza en vez de maximizar)
LL = function( p){
   R = dbinom(x = data, size = 1, prob = p)

   -sum(log(R))  # Negativo porque log de probabilidades es &lt;0.
 }

# Función que busca los parámetros que minimzan el negativo de la log verosimilitud
# Elegimos un valor inicial de p en el medio.
stats4::mle(LL, start = list(p = 0.5) )
</code></pre>
<pre><code>## 
## Call:
## stats4::mle(minuslogl = LL, start = list(p = 0.5))
## 
## Coefficients:
##         p 
## 0.6799996
</code></pre>
<p>Vemos que la estimación por MLE es <em>idéntica</em> a la media. No corresponde con el verdadero valor del parámetro poblacional p debido a la muestra particular que fue seleccionada.</p>
<p>Algunos comentarios finales:</p>
<ul>
<li>En algunos casos no es posible encontrar la solución óptima si no es por métodos numéricos.</li>
<li>Cuando <span class="arithmatex">\(n \to \infty\)</span> MLE converge en probabilidad al verdadero <span class="arithmatex">\(\theta\)</span>. Por ende cuando <span class="arithmatex">\(n \to \infty\)</span> el estimador bayesiano (que cumple la misma propiedad) y MLE serán muy parecidos entre sí y al verdadero <span class="arithmatex">\(\theta\)</span>.</li>
<li>MLE solo depende de las observaciones y no de cómo y en qué orden fueron recolectadas.</li>
</ul>

    <nav class="md-post__action">
      <a href="../../2019/10/31/maxima-verosimilitud-y-estimacion-bayesiana/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2019-10-13 00:00:00">2019-10-13</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../../category/estadistica/" class="md-meta__link">estadistica</a>, 
              <a href="../../category/r/" class="md-meta__link">R</a></li>
        
        
          
          <li class="md-meta__item">
            
              3 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="teorema-central-del-limite"><a class="toclink" href="../../2019/10/13/teorema-central-del-limite/">Teorema Central del Limite</a></h2>
<p>El <a href="https://es.wikipedia.org/wiki/Teorema_del_l%C3%ADmite_central">teorema central del límite (TCL)</a> 
es fundamental en el desarrollo de la estadística y ha obtenido
distintas variantes a lo largo de la historia. Veremos dos de las versiones más conocidas.</p>
<h4 id="teorema-central-del-limite-para-media-muestral-lindeberg-levy"><a class="toclink" href="../../2019/10/13/teorema-central-del-limite/#teorema-central-del-limite-para-media-muestral-lindeberg-levy">Teorema Central del Límite para Media Muestral (Lindeberg - Lévy)</a></h4>
<blockquote>
<p>Si las varaibles <span class="arithmatex">\(X_1 ... X_n\)</span> forman una muestra aleatoria de tamaño n proveniente de una 
distribución con media <span class="arithmatex">\(\mu\)</span> y varianza <span class="arithmatex">\(\sigma^2\)</span> (0 &lt; <span class="arithmatex">\(\sigma^2\)</span> &lt; <span class="arithmatex">\(\infty\)</span>), entonces  para 
cualquier número fijo x.
$$  \lim_{n\to \infty} Pr\Big[\frac{n^{1/2}(\bar X_n - \mu)}{\sigma} &lt;= x\Big] = \Phi (x)$$</p>
</blockquote>
<p>Donde <span class="arithmatex">\(\Phi (x)\)</span> es la función de distribución de una Normal Estándar.</p>
<p>El por qué de la convergencia del teorema no será probado acá pero no es díficil de encontrar.
Por ejemplo <a href="https://www.uv.es/ceaces/tex1t/2%20conver/levi.htm">ACÁ</a></p>
<p>Básicamente lo que dice el teorema, es que tomando una muestra grande de una población con media <span class="arithmatex">\(\mu\)</span> y 
varianza <span class="arithmatex">\(\sigma^2\)</span> definidas, entonces <span class="arithmatex">\(\frac{n^{1/2}(\bar X_n - \mu)}{\sigma}\)</span> va a tender a una normal estándar. Como consecuencia de eso podemos decir que <span class="arithmatex">\(\bar X_n\)</span> va a distribuirse 
aproximandamete como <span class="arithmatex">\(N(\mu, \sigma^2/n)\)</span>.</p>
<p>El TCL nos dice cómo se distribuye la media muestral si tenemos una muestra grande.</p>
<p>Análogamente, también podemos decir que <span class="arithmatex">\(\sum_{i=1}^n X_i\)</span> va a ser aproximadamente una normal
<span class="arithmatex">\(N(n\mu, n\sigma^2)\)</span></p>
<h5 id="ejemplo-lanzar-una-moneda"><a class="toclink" href="../../2019/10/13/teorema-central-del-limite/#ejemplo-lanzar-una-moneda">Ejemplo. Lanzar una moneda</a></h5>
<p>Si lanzamos una moneda 900 veces. Cuál es la probabilidad  de obtener más de 495 caras?</p>
<p><span class="arithmatex">\(X_i\)</span> = 1 si sale cara en el lanzamiento i, y 0 si sale cruz.<br />
E(<span class="arithmatex">\(X_i\)</span>) = 1/2 y Var(<span class="arithmatex">\(X_i\)</span>) = 1/4. Esto se deduce de ser un experimento con distribución Bernouilli.</p>
<p>Para llevarlo a los términos del TCL, tenemos una muestra de tamaño n = 900, con <span class="arithmatex">\(\mu\)</span> = 1/2 y
<span class="arithmatex">\(\sigma^2\)</span> = 1/4.</p>
<p>Por TCL tenemos que la distribución de la suma del número total de caras <span class="arithmatex">\(\sum_{i=1}^{900} X_i\)</span> se
distribuye aproximádamente como una normal con media = 900 * (1/2) = 450,
varianza = 900 * (1/4) = 225 y desvío estándar 225^(1/2) = 15.</p>
<p>Por lo tanto la variable <span class="arithmatex">\(Z = \frac{H - 450}{15}\)</span> se dsitribuye aproximadamente como una normal 
estándar.
<span class="arithmatex">\(<span class="arithmatex">\(Pr( H &gt; 495) = Pr(\frac{H - 450}{15} &gt; \frac{495 - 450}{15}) = Pr(Z&gt;3) = 1 - \Phi(3) = 0.0013\)</span>\)</span></p>
<p>Podemos comparar contra el resultado que obtenemos al hacer el mismo ejercicio pero mirando 
directamente la distribución binomial (que es la que realmente genera el proceso de datos)</p>
<pre><code class="language-r">pbinom(495,900, 0.5, lower.tail = FALSE)
</code></pre>
<pre><code>## [1] 0.001200108
</code></pre>
<p>Vemos que los resultados son muy similares.</p>
<h4 id="teorema-central-del-limite-para-suma-de-variables-aleatorias-independientes-liapunov"><a class="toclink" href="../../2019/10/13/teorema-central-del-limite/#teorema-central-del-limite-para-suma-de-variables-aleatorias-independientes-liapunov">Teorema Central del Límite para Suma de Variables Aleatorias Independientes (Liapunov)</a></h4>
<p>Este TCL aplica a una secuencia de variables aleatorias independientes pero que no necesariamente
tienen que provenir de una misma distribución. Todas deben tener una media y varianza definidas.</p>
<p>La variable <span class="arithmatex">\(<span class="arithmatex">\(Y_n = \frac{\sum_{i=1}^n X_i - \sum_{i=1}^2 \mu_i}{(\sum_{i=1}^n\sigma_i^2)^{1/2}}\)</span>\)</span></p>
<p>Entonces <span class="arithmatex">\(E(Y_n) = 0\)</span> y <span class="arithmatex">\(Var(Y_n)\)</span> = 1</p>
<p>Siendo un poco más precisos veamos el teorema:</p>
<blockquote>
<p>Suponiendo que las variables aleatorias <span class="arithmatex">\(X_1. X_2, ...\)</span>  son independientes y que
<span class="arithmatex">\(E(|X_i - \mu_i|^3) &lt; \infty\)</span> para 1,2,...
Y suponidendo que <span class="arithmatex">\(<span class="arithmatex">\(\lim_{n\to \infty} \frac{\sum_{i=1}^n E(|X_i - \mu_i|^3)}{(\sum_{i=1}^n \sigma^2_i)^{3/2}} = 0\)</span>\)</span>
Entonces, utilizando la variable Y definida previamente tenemos que <span class="arithmatex">\(<span class="arithmatex">\(\lim_{n \to \infty} Pr(Y_n &lt;= x) = \Phi(x)\)</span>\)</span></p>
</blockquote>
<p>La interpretacaión del teorema es que si se cumple la condición de los 3eros momentos, entonces para valores grandes de n la distribución de <span class="arithmatex">\(\sum_{i=1}^n X_i\)</span> será aproximadamente normal con media <span class="arithmatex">\(\sum_{i=1}^n \mu_i\)</span> y varianza <span class="arithmatex">\(\sum_{i=1}^n \sigma^2_i\)</span>.</p>
<h4 id="diferencias-entre-lindeberg-levy-y-liapunov"><a class="toclink" href="../../2019/10/13/teorema-central-del-limite/#diferencias-entre-lindeberg-levy-y-liapunov">Diferencias entre Lindeberg-Lévy y Liapunov</a></h4>
<p>El teorema de Lindeberg-Lévy aplica para secuencias de variables aleatorias iid y solo requiere que la varianza de estas variables sea finita. En cambio el teorema de Liapunov aplica a secuencias de variables aleatorias independientes pero que no necesariamente provienen de una misma distribución. Requiere que el tercer momento de cada variable existe y cumple con la ecuación del teorema.</p>
<h4 id="efecto-de-tcl"><a class="toclink" href="../../2019/10/13/teorema-central-del-limite/#efecto-de-tcl">Efecto de TCL</a></h4>
<p>Más allá de la utilidad para aproximar distribuciones y medias mediante una normal, el TCL aporta una posible explicación a por qué tantas variables se distribuyen aproximadamante como normales. Si muchas de las variables a medir pueden pensarse como sumas de otras variables es lógico que tiendan a verse como normales aunque las variables que se suman para darle origen provengan de distintas distribuciones.</p>

    <nav class="md-post__action">
      <a href="../../2019/10/13/teorema-central-del-limite/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2019-07-01 00:00:00">2019-07-01</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../../category/estadistica/" class="md-meta__link">estadistica</a>, 
              <a href="../../category/r/" class="md-meta__link">R</a></li>
        
        
          
          <li class="md-meta__item">
            
              11 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="metodos-de-resampleo-islr-capitulo-5"><a class="toclink" href="../../2019/07/01/metodos-de-resampleo/">Metodos de Resampleo - ISLR Capitulo 5</a></h2>
<p>Los métodos de resampleo son indispensables en la estadística moderna ya que permiten ajustar modelos a diferentes muestras de un mismo set de entrenamiento con el fin de obtener mayor información del modelo. Por ejemplo puede ser de utilidad para ver la variabilidad del modelo en distintas muestras. Los dos métodos que se presentan en el capítulo son <em>cross-validation</em> y <em>bootstrap</em>. A grandes rasgos CV puede servir para estimar el test error de un modelo o para ajustar hiperparámetros del modelo como el nivel de flexbilidad. Por su parte bootstrap puede usarse para medir la precisión de un parámetro estimado mediante un modelo estadístico.</p>
<h3 id="cross-validation"><a class="toclink" href="../../2019/07/01/metodos-de-resampleo/#cross-validation">Cross-Validation</a></h3>
<p>De los modelos que uno entrena es de sumo interés obtener el "test error" que sería el error promedio al predecir una nueva observación aplicando el modelo estadístico entrenado. Esto puede calcularse si tenemos un set de testeo puntualmente para ello pero no suele ser el caso lamentablemente.
En general no se tienen tantos datos como para separar en sets como uno desearía y surgen distintas técnicas para estimar el test error basado solamente en los datos de entrenamiento. Algunas de estas técnicas estiman el test error ajustando el training error por algún factor mientras que otras separan el training set en subsets donde uno hace las veces de test set.</p>
<h5 id="validation-set"><a class="toclink" href="../../2019/07/01/metodos-de-resampleo/#validation-set">Validation Set</a></h5>
<p>Un método muy utilizado es el del set de validación. Básicamente consiste en separar nuestro training set en dos sets, un "nuevo" training set y uno set de validación. Una práctica habitual es separar 70-30, pero va a depender de la cantidad de observaciones que tengan y no hay una regla estricta. Básicamente de los datos que tienen para entrenar el modelo separan una parte que va a ser el set de validación y entrenan el modelo con los datos restantes (70% por ejemplo). Luego se mide la precisión del modelo en el 30% restante (set de validación) que  son datos que no fueron utilizados a la hora de ajustar el modelo. Si utilizamos el MSE (mean squared error) cómo medida del error, este va a ser nuestro test error estimado. Recordemos que es el MSE calculado con las predicciones en el set de validación.
Por otra parte el set de validación también puede servir para ajustar algún hiperparámetro. Se pueden correr muchos modelos con distintos hiperparámetros y ver cuál tiene menor MSE en el set de validación.  </p>
<p>Es un método muy sencillo y suele ser eficaz pero tiene dos potenciales problemas:
* El MSE puede variar mucho según cómo dividieron las observaciones en training y validación. Otra segmentación puede dar resultados muy distintos.
* No utilizás todos tus datos para ajustar el modelo y puede que eso lleve a sobreestimar el test error, que quizás sería menor si usaras todas las observaciones para entrenar el modelo.</p>
<h5 id="leave-one-out-cross-validation"><a class="toclink" href="../../2019/07/01/metodos-de-resampleo/#leave-one-out-cross-validation">Leave-One-Out Cross-Validation</a></h5>
<p>LOOCV es un intento de solucionar los problemas del enfoque del set de validación [<strong>SPOILER: No es recomendado pero vale la pena conocerlo</strong>].<br />
Este enfoque es llevar el set de validación al extremo. Lo que se hace es de nuevo separar nuestro training set en dos pero esta vez guardando una sola observación como validación y usando las n-1 restantes para entrenar el modelo. La idea es hacer esto n veces, dejando cada vez una observación distinta como validación.
El test error estimado es el promedio de los MSE de cada predicción que se hizo de la observación de validación. 
Pensando en los problemas del set de validación, con LOOCV logramos usar casi todos los datos disponibles para entrenar el modelo (n-1 observaciones) por lo tanto deberíamos tener modelos con menos sesgo y no sobreestimar tanto el test error como con el enfoque de set de validación.
Por otra parte con el set de validación podemos obtener resultados muy distintos según el azar de cómo dividamos nuestros datos. En LOOCV esto no pasa ya que todos nuestros modelos de entrenamiento van a ser practicamente iguales salvo por una observación cada vez. No hay azar en la división de training y validación. 
Enseguida vemos el mayor problema de este enfoque, que es computacional. Debemos ajustar n modelos y no solo uno. Dependiendo de nuestros datos y la complejidad de nuestro modelo esto puede demandar muchísimo tiempo/recursos.</p>
<h5 id="k-fold-cross-validation"><a class="toclink" href="../../2019/07/01/metodos-de-resampleo/#k-fold-cross-validation">K-Fold Cross-Validation</a></h5>
<p>K-Fold CV es un punto intermedio entre ambos enfoques y es de lo más utilizado al día de hoy. Consiste en separar nuestros datos en K subsets de mismo tamaño. Se selecciona uno de esos K subsets y se lo deja como validación. Se entrena el modelo con los K-1 subsets y se predice en el de validación que separamos. Así K veces, dejando como validación cada vez uno subset distinto. El Test error estimado es el promedio de los MSE en cada caso. Se puede ver fácilmente que si K = n, entonces estaríamos en LOOCV. Los valores típicos de K suelen ser 5 o 10, y por ende es muchísimo menos costoso que LOOCV.
Al separar en "solo" 10 subsets cada set de validación puede tener cierta variabilidad en el MSE respecto a otros pero esta va a ser menor que en el enfoque de set de validación. En el libro se muestran unos gráficos para data simulada donde se ve que LOOCV y K-Fold tienen comportamiento muy similar y según el caso pueden sobreestimar o subestimar el verdadero test error (depende el problema y la flexibilidad elegida).
Como mencionamos para el set de validación, K-fold también puede ser utilizado para ajustar algún hiperparámetro del modelo como el nivel de fleixibilidad. En este caso lo que nos interesa es encontrar el valor mínimo del MSE entre los distintos posibles valores del hiperparámetro para decidir cual es el mejor posible pero el valor puntual del MSE o su precisión no nos interesa tanto.</p>
<h5 id="trade-off-entre-sesgo-y-varianza-en-k-fold-cross-validation"><a class="toclink" href="../../2019/07/01/metodos-de-resampleo/#trade-off-entre-sesgo-y-varianza-en-k-fold-cross-validation">Trade-Off entre sesgo y varianza en K-Fold Cross-Validation</a></h5>
<p>Otro punto muy importante de K-Fold, además de que requiere menos intensidad computacional que LOOCV, es que suele dar estimaciones más precisas del test error que LOOCV, y esto tiene que ver por el tradeoff entre sesgo y varianza.</p>
<p>Vimos antes que LOOCV debería ser el estimar más insesgado del test error ya que utiliza casi todas las observaciones de entrenamiento cada vez sin embargo hay que ver que sucede con la varianza ya que es otro componente del MSE. (Más detalles en <a href="https://fbetteo.netlify.com/2019/05/aprendizaje-estad%C3%ADstico-islr-capitulo-2/">ISLR Cap 2</a>).<br />
Resulta que LOOCV tiene mayor varianza que K-Fold CV siempre que K sea menor que n. Esto sucede porque en LOOCV lo que hacemos es promediar el resultado de n modelos cuyos datos de entrenamiento son casi idénticos (salvo por una observación) y por ende los resultados están en gran medida correlacionados positivamente.<br />
Por otro lado al hacer K-Fold CV se promedian <em>solo</em> K resultados que están menos correlacionados entre sí ya que los datos de entrenamiento se solapan menos entre ellos. 
La clave acá es que el promedio de muchos valores altamente correlacionados tiene mayor varianza que el promedio de muchos valores que no están tan correlacionados. Dado este escenario se hicieron pruebas que llegaron a la conclusión empírica de que K=5 y k = 10 son valores que no suelen tener excesivo sesgo ni varianza.<br />
Al parecer en los últimos años se empezó a dudar de la universalidad de este enunciado y se han hecho pruebas donde LOOCV no tiene mayor varianza. Sin embargo sigue siendo computacionalmente más demandante y el beneficio del menor sesgo no era suficiente para darle demasiada importancia.</p>
<h5 id="cross-validation-en-problemas-de-clasificacion"><a class="toclink" href="../../2019/07/01/metodos-de-resampleo/#cross-validation-en-problemas-de-clasificacion">Cross-Validation en problemas de clasificación.</a></h5>
<p>Los procedimientos vistos hasta ahora son útiles tanto para variables continuas como para problemas de clasificación. Vinimos usando ejemplos donde la medida del error era el MSE (variable dependiente continua) pero podemos aplicar todo de la misma manera utilizando alguna medida de clasificación como la cantidad de observaciones mal clasificadas. Todo el resto se mantiene y es válido, tanto sete de validación, como LOOCV o K-Fold.</p>
<h3 id="bootstrap"><a class="toclink" href="../../2019/07/01/metodos-de-resampleo/#bootstrap">Bootstrap</a></h3>
<p>El <em>bootstrap</em> es una herramienta estadística muy extendida que se utiliza para cuantificar la incertidumbre asociada a algún estimador o método de aprendizaje estadístico. Un ejemplo sencilla sería que se puede usar para estimar los errores estándar de los coeficientes de una regresión lineal. Sin embargo lo poderoso de esta herramienta es que es utilizable en muchísimos métodos de aprendizaje, incluso en algunos donde es difícil estimar la varianza o esta no es calculado por los paquetes estadísticos.<br />
Idealmente para estimar la variabilidad de un estimador lo que uno haría es ajustar un modelo n veces y ver cómo varía el estimador a lo largo de esos n modelos utilizando n muestras. Sin embargo no es habitual tener tantos datos ni muestras disponibles. Mismo uno querría utilizar todos los datos en simultáneo posiblemente para reducir el sesgo. Acá es donde bootstrap se luce ya que permite emular el proceso de obtener nuevas muestras de datos a partir de nuestros datos de entrenamiento. <strong>En vez de muestrear de manera independiente sobre la población lo que se hace es muestrear n veces con reposición de nuestro set de entrenamiento, generado n muestras a partir de nuestros datos originales</strong>.
Ya con nuestras nuevas muestras (provenientes todas del dataset original) podemos calcular n modelos y por ende n veces el mismo estimador, pudiendo estimar el desvío estándar de este.<br />
En el fondo lo que se hace es suponer que nuestra muestra es <em>representativa</em> de la población y es nuestra mejor aproximación. Luego obtenemos muestras de estos datos que son nuestra versión reducida de la población. Posiblemente haya algún sesgo pero es una herramienta bastante útil para estimar la variabilidad de nuestros estimadores.</p>
<p>Generamos un ejemplo para ver cómo funciona.</p>
<p>Empezamos generado una población de y que depende x con intercepto 5 y b1 = 5.</p>
<pre><code class="language-r">library(ggplot2)
set.seed(1)
x &lt;-rnorm(10000, mean = 2, sd = 3)
y &lt;- 4 + 5*x + rnorm(10000,0,4)
df &lt;- cbind.data.frame(y,x)

ggplot(data = df, aes( x =x, y =y )) + 
  geom_point()
</code></pre>
<p><img alt="Image" src="../../img/2019-07-01-metodos-de-resampleo-unnamed-chunk-1-1.png" /></p>
<p>Primero vemos el caso ideal que sería poder obtener muchas muestras de la población y ajustar modelos a estas. Luego veremos como varían nuestros coeficientes.</p>
<pre><code class="language-r"># Muestras de la población
results_pop &lt;- data.frame(b0 = double(), b1 = double())
set.seed(123)
for (i in 1:1000){
  df_train &lt;- df[sample(nrow(df),size = 500,replace = FALSE),]
  ml_train &lt;- lm(formula = y ~ x, data = df_train)
  results_pop[i,1] = ml_train$coefficients[[1]]
  results_pop[i,2] = ml_train$coefficients[[2]]

}

summary(results_pop)
</code></pre>
<pre><code>##        b0              b1       
##  Min.   :3.198   Min.   :4.804  
##  1st Qu.:3.834   1st Qu.:4.969  
##  Median :3.974   Median :5.005  
##  Mean   :3.975   Mean   :5.004  
##  3rd Qu.:4.115   3rd Qu.:5.043  
##  Max.   :4.615   Max.   :5.169
</code></pre>
<pre><code class="language-r">print(paste0(&quot;El desvío estándar de b0 a partir de 1000 modelos es &quot;, sd(x = results_pop$b0)))
</code></pre>
<pre><code>## [1] &quot;El desvío estándar de b0 a partir de 1000 modelos es 0.207882713489026&quot;
</code></pre>
<pre><code class="language-r">ggplot(data = results_pop) + 
  geom_histogram( aes( x = b0), fill = &quot;white&quot;, colour = &quot;black&quot;)  + 
  geom_vline( aes(xintercept = mean(b0)), colour = &quot;red&quot;, size = 1)
</code></pre>
<p><img alt="Image" src="../../img/2019-07-01-metodos-de-resampleo-unnamed-chunk-2-1.png" /></p>
<p>Vemos que estimando 1000 modelos a partir de 500 observaciones independientes de la población original obtenemos para b0 estimaciones centradas aproximadamente en el valor real (3.975) pero con un mínimo encontrado en 3.198 y un máximo en 4.615. El desvío estándar de la estimación es de 0.2078.
A su vez mostramos un histograma de cómo se distribuye la estimación de b0.</p>
<p>Ahora simulemos un caso real donde solo tenemos una muestra de 500 observaciones y es todo con lo que podemos trabajar.
Como primera medida estimamos una regresión lineal y vemos qué parámetros ajustan mejor nuestros datos.</p>
<pre><code class="language-r"># Muestras de la población
results_sample &lt;- data.frame(b0 = double(), b1 = double())
set.seed(456)
df_train_sample &lt;- df[sample(nrow(df),size = 500,replace = FALSE),]
ml_train_sample &lt;- lm(formula = y ~ x, data = df_train_sample)

results_sample[1,1] &lt;- ml_train_sample$coefficients[[1]]
results_sample[1,2] &lt;- ml_train_sample$coefficients[[2]]

knitr::kable(results_sample, caption = &quot;Coefficients&quot;)
</code></pre>
<p>Table: Coefficients</p>
<table>
<thead>
<tr>
<th style="text-align: right;">b0</th>
<th style="text-align: right;">b1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">3.89621</td>
<td style="text-align: right;">5.025914</td>
</tr>
</tbody>
</table>
<p>Vemos que a partir de entrenar el modelo con las 500 observaciones obtenemos un intercepto de 3.896 y un b1 estimado de 5.026.
Nosotros, como conocemos la población, sabemos que el intercepto no es del todo preciso ya que el real es 4 sin embargo en un caso real eso no lo sabríamos. Nos interesaría saber qué variabilidad tiene ese coeficiente para tener una medida de qué tan variable es nuestro resultado.<br />
Para una regresión lineal eso se puede saber ya que no es difícil calcular la varianza de los estimadores, pero con modelos más complicados no siempre se puede y ahí es donde bootstrap ayuda realmente. Acá lo hacemos con la regresión lineal porque es lo más sencillo de mostrar.<br />
Suponiendo que queremos obtener una estimaación de la variabilidad del coeficiente estimado b0 procedemos con bootstrap.</p>
<p>Fijense que lo que hacemos es distinto al primer caso. Acá tomamos 10000 muestras no de la población sino de nuestro set de 500 observaciones. Estas muestras son también de 500 observaciones, la diferencia es que es con reposición por lo tanto una misma observación puede figurar más de una vez.</p>
<pre><code class="language-r"># Muestras de la población
results_bootstrap &lt;- data.frame(b0 = double(), b1 = double())
set.seed(789)
for (i in 1:10000){
  df_train_bs &lt;- df[sample(nrow(df_train_sample),size = 500,replace = TRUE),]
  ml_train_bs &lt;- lm(formula = y ~ x, data = df_train_bs)
  results_bootstrap[i,1] = ml_train_bs$coefficients[[1]]
  results_bootstrap[i,2] = ml_train_bs$coefficients[[2]]

}

summary(results_bootstrap)
</code></pre>
<pre><code>##        b0              b1       
##  Min.   :3.245   Min.   :4.791  
##  1st Qu.:3.822   1st Qu.:4.982  
##  Median :3.962   Median :5.020  
##  Mean   :3.962   Mean   :5.019  
##  3rd Qu.:4.100   3rd Qu.:5.057  
##  Max.   :4.709   Max.   :5.205
</code></pre>
<pre><code class="language-r">print(paste0(&quot;El desvío estándar de b0 a partir de 10000 modelos es &quot;, sd(x = results_bootstrap$b0)))
</code></pre>
<pre><code>## [1] &quot;El desvío estándar de b0 a partir de 10000 modelos es 0.207126704818891&quot;
</code></pre>
<pre><code class="language-r">ggplot(data = results_bootstrap) + 
  geom_histogram( aes( x = b0),fill = &quot;white&quot;, colour = &quot;black&quot;) + 
  geom_vline( aes(xintercept = mean(b0)), colour = &quot;blue&quot;, size = 1)
</code></pre>
<p><img alt="Image" src="../../img/2019-07-01-metodos-de-resampleo-unnamed-chunk-4-1.png" /></p>
<p>Voilà. Corrimos 10000 iteraciones de nuestro modelo a partir de 10000 muestras de nuestra data original. El desvío estándar de b0 para bootstrap quedó de 0.2071. Que si comparamos con el de 1000 muestras independientes que era 0.2078 es prácticamente igual.
A su vez, podemos calcular el desvío teórico de b0 a partir del modelo (la solución fácil).</p>
<pre><code class="language-r">summary(ml_train_sample)
</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = df_train_sample)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.2193  -2.8265   0.0281   2.7421  10.7577 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.89621    0.21607   18.03   &lt;2e-16 ***
## x            5.02591    0.05902   85.16   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.98 on 498 degrees of freedom
## Multiple R-squared:  0.9357, Adjusted R-squared:  0.9356 
## F-statistic:  7252 on 1 and 498 DF,  p-value: &lt; 2.2e-16
</code></pre>
<p>Vemos que desde R el modelo nos devuelve que b0 tiene un desvío de 0.21607. Prácticamente igual al desvío de las muestras independientes como al de bootstrap.
Por otra parte vemos que el promedio de b0 estimado en bootstrap es mucho más cercano a 4 que el estimado con una sola iteración y quedó mucho más cerca que el promedio de los estimados mediante muestras independientes.
Nada mal no?</p>

    <nav class="md-post__action">
      <a href="../../2019/07/01/metodos-de-resampleo/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2019-05-25 00:00:00">2019-05-25</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../../category/estadistica/" class="md-meta__link">estadistica</a>, 
              <a href="../../category/r/" class="md-meta__link">R</a></li>
        
        
          
          <li class="md-meta__item">
            
              13 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="regresion-lineal-islr-capitulo-3"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/">Regresion Lineal - ISLR Capítulo 3</a></h2>
<p>La regresión lineal simple es un método muy directo para estimar una variable cuantitativa Y en base a un solo predictor X. Asume que hay una relación lineal entre X e Y.
$$ Y \approx \beta_0 + \beta_1X$$
<span class="arithmatex">\(\beta_0\)</span> y <span class="arithmatex">\(\beta_1\)</span> son dos constantes desconocidas que representan al <em>intercepto</em> y a la <em>pendiente</em> del modelo lineal. Son los coeficientes o parámetros. Con nuestros datos podemos estimar coeficientes para predecir futuros valores de Y basados en X y nuestro modelo.</p>
<h4 id="estimacion-de-coeficientes"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#estimacion-de-coeficientes">Estimación de Coeficientes</a></h4>
<p>Los coeficientes que buscamos son <span class="arithmatex">\(\hat \beta_0\)</span> y <span class="arithmatex">\(\hat \beta_1\)</span> (estimados, por eso el sombrero) son aquellos que generen una recta que pase lo más cerca posible de todos nuestros datos de entrenamiento. Hay varias manera de definir "cerca" pero la más usada es el enfoque de <em>mínimos cuadrados</em>.</p>
<p>Supongamos un ejemplo donde tenemos datos de horas trabajadas por ciertos individuos y la paga que reciben. Supongamos a fines del ejemplo que la relación entre salario y horas es lineal (sabemos que no es real...)</p>
<p>Cuando estimemos <span class="arithmatex">\(\hat \beta_0\)</span> y <span class="arithmatex">\(\hat \beta_1\)</span> obtendremos después un valor <span class="arithmatex">\(\hat y_i\)</span> para cada valor de <span class="arithmatex">\(x_i\)</span> (cada observación), que será el resultado de la predicción de nuestro modelo para ese valor de horas trabajadas.
$$ \hat y_i = \hat \beta_0 + \hat \beta_1 x_i$$
Luego <span class="arithmatex">\(e_i = y_i - \hat y_i\)</span> representa el <em>residuo</em>, que es la diferencia entre el valor real del salario para esa observación y el valor que predice nuestro modelo.<br />
Una métrica importante a saber es la <em>suma de resiudos al cuadrado</em> (RSS por siglas en inglés) que es:
$$ RSS = e_1^2 + e_2^2 + ... + e_n^2$$
o de manera equivalente:
$$ RSS = (y_1 - \hat \beta_0 - \hat \beta_1x_1) ^2 +  (y_2 - \hat \beta_0 - \hat \beta_1x_2) ^2  + ... + (y_n - \hat \beta_0 - \hat \beta_1x_n) ^2 $$</p>
<p>Que basicamente es la suma de todas las diferencias entre lo predicho por nuestro modelo y el dato real de nuestro set, elevadas al cuadrado. Esto último es principalmente para evitar que se compensen los errores. Sobreestimar por 10 y luego subestimar por 10 tiene como suma de errores 0. Si elevamos esas diferencias al cuadrado, todos los errores serán positivos y se acumularán. En este caso seria <span class="arithmatex">\(10^2\)</span> + <span class="arithmatex">\((-10)^2\)</span>, que es 200.<br />
El enfoque de mínimos cuadrados estima <span class="arithmatex">\(\hat \beta_0\)</span> y <span class="arithmatex">\(\hat \beta_1\)</span> de tal manera que el RSS sea el mínimo posible dados los datos.  </p>
<p>Usando un poco de cálculo se puede demostrar que los parámetros que minimizan RSS son:
$$ \hat \beta_1 = \frac{\sum_{i = 1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}$$
$$ \hat \beta_0 = \bar{y} - \hat \beta_1\bar{x}$$
donde <span class="arithmatex">\(\bar{y}\)</span> y <span class="arithmatex">\(\bar{x}\)</span> son las respectivas medias muestrales.</p>
<p><img alt="Image" src="../../img/2019-05-15-regresion-lineal-islr-reg1-1.png" /></p>
<p>En nuesto caso usando este set de datos generado ficticiamente obtenemos <span class="arithmatex">\(\hat \beta_0\)</span> = 2.000713 &times; 10<sup>4</sup><br />
y <span class="arithmatex">\(\hat \beta_1\)</span> = 299.82</p>
<p>Recordemos que esta es una estimación en base a los datos y no sabemos los verdaderos parámetros de la DGP real(proceso generador de datos). En este caso yo si lo sé porque generé los datos pero en la vida real es inaccesible.<br />
Lo que hicimos fue estimar, a partir de un set de datos, ciertos coeficientes o característica de una población mucho más amplia. (Todos los trabajadores del país..)</p>
<h4 id="precision-de-nuestros-estimadores"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#precision-de-nuestros-estimadores">Precisión de nuestros estimadores</a></h4>
<p>En este caso teníamos una sola muestra pero podríamos haber tenido muchas muestras (K) de la misma población (muchos sets de datos con horas trabajadas y salarios).<br />
Si estimáramos los coeficientes para cada uno de esos sets obtendríamos K pares de coeficientes, cada uno calculado con las particularidades de esos sets.<br />
Se puede demostrar que el promedio de una cantidad grande de estimadores provenientes de muchas muestras se centra en el verdadero valor poblacional (si el modelo es correcto). Es decir que el promedio de los K <span class="arithmatex">\(\hat \beta_1\)</span> va a centrarse en el verdadero valor poblacional de <span class="arithmatex">\(\beta_1\)</span> ( y lo mismo para <span class="arithmatex">\(\beta_0\)</span>).<br />
Pero estos K parámetros centrados en el verdadero valor van a tener cierta dispoersión, es decir, pueden estar todos muy cerca del verdadero o estar muy dispersos pero que en promedio si quede centrado. Esto determina que tan preciso es el coeficiente que estimemos de una muestra.  Este desvío estándar de los parámetros (SE) puede estimarse y depende de la varianza del error del modelo.</p>
<p>Puede ser útil para calcular los <em>intervalos de confianza</em> de los parámetros. Estos son intervalos que con X% de probabilidad contienen al verdadero valor del parámetro poblacional. Lo más habitual es calcular el intervalo de confianza al 95%.
Para <span class="arithmatex">\(\hat \beta_1\)</span> esto es aproximadamente:
$$ \hat \beta_1 \pm 2 \cdot SE(\hat \beta_1)$$
La interpretación sería que de 100 intervalos que construya de esta manera (de 100 muestras distintas), 95 van a tener al verdadero valor de <span class="arithmatex">\(\beta_1\)</span>.  </p>
<p>Por otra parte podemos realizar un test de hipotésis de los coeficientes. El más común es testear la siguiente hipótesis nula:<br />
<em>H_0</em> : No hay relación entre X e Y<br />
contra la hipótesis alternativa<br />
<em>H_1</em> : Hay alguna relación entre X e Y<br />
Lo cual se traduce en:
$$ H_0 : \beta_1 = 0 $$
$$ H_1 : \beta_1 \neq 0$$</p>
<p>Lo que se hace es determinar si <span class="arithmatex">\(\hat \beta_1\)</span> está lo suficientemente lejos de 0 como para rechazar la hipótesis nula. Qué tan lejos es suficiente depende en gran parte del desvío estándar (SE) del coeficiente. Si el SE es grande , necesitaremos valores elevado de <span class="arithmatex">\(\hat \beta_1\)</span> para estar tranquilos con que el valor real no puede ser 0.<br />
Para esto lo que se hace es calcular el estadístico <em>t</em>:
$$ t = \frac{\hat \beta_1 - 0}{SE(\hat \beta_1)}$$
Que mide cuantos desvíos estándar <span class="arithmatex">\(\hat \beta_1\)</span> está alejado de 0. Si no hay relación entre X e Y se espera que el estadístico tenga una distribución t con n - 2 grados de libertad. Dado ese supuesto, lo que se hace es calcular la probabilidad de obtener un valor de t como el de nuestro estadístico, si este proviene de una distribución t con n-2 grados de libertad. Esta probabilidad se la conoce como  <em>p valor</em>.
Sería qué tan probable es encontrar un valor al menos tan grande como el de t si este proviniera de la distribución t con n-2 grados de libertad. Si esta probabilidad es muy chica (el umbral habitual es 0.05 pero depende del trabajo) uno rechaza la hipótesis nula en favor de la alternativa, suponiendo que sí hay una relación entre X e Y.</p>
<h4 id="precision-del-modelo"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#precision-del-modelo">Precisión del modelo.</a></h4>
<p>Naturalmente a uno le interesa saber qué tan bien ajusta nuestro modelo a los datos.</p>
<p>El método más habitual para regresión lineal es el R^2. Toma valores entre 0 y 1 porque es la proporción de la varianza de Y explicada por nuestro modelo.</p>
<div class="arithmatex">\[ R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}\]</div>
<p>donde $TSS = \sum (y_i -\bar{y})^2 $ es la suma de cuadrados totales y RSS es la suma de errores cuadrados que ya definimos antes. TSS mide la varianza total de Y y representa la variabilidad total inherent de la variable dependiente <em>antes</em> de correr la regresión. Por el contrario RSS mide la variabilidad que queda sin explicar por nuestro modelo (recuerden que proviene de los residuos).  Por lo tanto el numerador TSS - RSS mide la parte de la variabilidad de Y que sí pudo ser explicada por el modelo, y lo divide por la variabilidad total. <span class="arithmatex">\(R^2\)</span> mide entonces la proporción de la variablidad que pudo ser explicada usando X. Cuanto más cerca de 1, mejor.</p>
<h4 id="regresion-con-multiples-predictores"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#regresion-con-multiples-predictores">Regresión con Múltiples Predictores</a></h4>
<p>Suena mucho más lógico tratar de explicar una variable dependiente no solo por una independiente si no por varias. La regresión lineal simple puede ampliarse a regresión lineal múltiple donde nuestro modelo pasa a ser:
$$ \hat y_i = \hat \beta_0 + \hat \beta_1 x_i + ... + \hat \beta_p x_p$$
Y mantenemos un término de error con distribución normal y media 0.</p>
<p>En esencia la idea es la misma, explicar la variabilidad de Y basado en la variabilidad de nuestros predictores. La metodología para estimar los coeficientes suele ser Mínimos Cuadrados como vimos antes, sin embargo la solución no suele ser tan fácil de expresar y es más sencillo verlo en términos matriciales o simplemente ver los resultados desde el programa estadístico que estemos usando.
No olvidar que varias regresiones simples no pueden sumarse en una resgresión múltiple, es decir, los coeficientes de las regresiones simples no tienen por que ser los mismos ni por qué mantener el signo cuando se juntan todas las variables en un solo modelo. Esto sucede porque la regresión múltiple estima coeficientes <em>controlando</em> por todas las otras variables, es decir, quitando el efecto de las otras. Por eso es que por separado quizás dos variables son significativas pero en una regresión múltiple solo una de ellas lo es. En general esto viene dado porque están correlacionadas y se comportan de manera similar. Al final del día no es fácilmente distinguible cuál es realmente la que lidera el efecto.</p>
<h5 id="hay-relacion-entre-la-dependiente-y-los-predictores-test-f"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#hay-relacion-entre-la-dependiente-y-los-predictores-test-f">Hay relación entre la dependiente y los predictores? (test F)</a></h5>
<p>En regresión simple vimos el test de hipótesis para ver si el coeficiente de <span class="arithmatex">\(\hat \beta_1\)</span> era significativamente distinto de 0. En regresión múltiple lo que debemos hacer es chequear si <em>todos</em> nuestros coeficientes son distintos de 0. (Y no uno por uno)</p>
<p>Lo cual se traduce en:
$$ H_0 : \beta_1 = \beta_2 = ... = \beta_p = 0 $$
$$ H_1 : \text{Al menos algún } \beta_j \neq 0$$
El test de hipótesis se hace calculando el estadístico F.
$$ F = \frac{(TSS - RSS) / p}{RSS / (n - p -1)}$$</p>
<p>Si los supuestos del modelo lineal se cumplen puede probarse que <span class="arithmatex">\(E[RSS/(n - p-1)] = \sigma^2\)</span> y si <span class="arithmatex">\(H_0\)</span> es verdadera <span class="arithmatex">\(E[(TSS-RSS)/p] = \sigma^2\)</span>. Por lo tanto si no hay relación el estadístico F será cercano a 1 y si en realidad la hipótesis alternativa es verdadera el numerador será mayor que <span class="arithmatex">\(\sigma^2\)</span> y por lo tanto F será mayor que 1. Dependiendo de n, p y del nivel de significatividad que busquemos F deberá superar un umbral distinto para poder rechazar la hipótesis nula.<br />
Es inevitable mirar los p-valores individuales sin embargo debemos tener cuidado particularmente cuando tenemos muchas variables. Por definición, algunos coeficientes saldrán significativos <em>por azar</em> aunque no tengan relación con la variable dependiente. En el caso típico de significativdad del 95%, esto sucede en promedio el 5% de las veces. Con muchas variables nuestras posibilidades de encontrarnos con al menos algún falso significativo aumentan notoriamente por lo que hay que mirar con cuidado. Por su parte, el estadístico F corrige en su cálculo por la cantidad de coeficientes y por lo tanto no se ve afectado por este problema.</p>
<h5 id="seleccion-de-variables"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#seleccion-de-variables">Selección de variables</a></h5>
<p>Cuando tenemos un set de datos grande es habitual tener que seleccionar cuáles son las variables importantes para el modelo. Más allá del conocimiento del problema (fundamental), idealmente lo mejor es probar una gran cantidad de modelos y con alguna métrica de comparación seleccionar los mejores. El problema es que la cantidad de modelos posible crece exponencialmente con la cantidad de variables y esto no es posible.<br />
En el libro los autores mencionan como alternativas Forward Selection, Backward Selection y Selección mixta. Básicamente son enfoques que prueban una muestra de todos los modelos posibles según la significatividad de las variables. Son métodos iterativos. Habiendo avanzado la disciplina, llegado el caso buscaría otros métodos vigentes para atacar este problema.</p>
<h5 id="ajsute-del-modelo"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#ajsute-del-modelo">Ajsute del modelo</a></h5>
<p>Para verificar el ajuste del modelo se sigue usando el <span class="arithmatex">\(R^2\)</span> como métrica principal. En este caso es equivalente a la correlación al cuadrado de Y e <span class="arithmatex">\(\hat Y\)</span>. Un punto a tener en cuenta es que el R^2 nunca puede disminuir al agregar variables ya que el peor escenario posible es que la nueva variable tenga coeficiente de 0 y el ajuste quede igual que antes. Lo que se hace para controlar por esto y poder comparar modelos es ajustar el <span class="arithmatex">\(R^2\)</span> por la cantidad de variables utilizadas o usar el RSE. De cualquier manera lo importante es recordar que el R2 sigue siendo útil en la regresión lineal múltiple.</p>
<h4 id="otras-consideraciones"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#otras-consideraciones">Otras consideraciones</a></h4>
<ul>
<li>
<p>Las variables independientes admiten variables categóricas! (Binarias o multiclase). Ej: Educación máxima alcanzada.
Hay que mirar con atención la interpretación. Alteran el intercepto según la categoría de la observación y puede alterar pendientes si se las incluye en interacción con alguna variable continua.</p>
</li>
<li>
<p>El modelo que venimos viendo el aditivo y lineal, pero podemos remover esos supuestos. Por ejemplo podemos agregar interacción entre variable y por lo tanto relajar la aditividad. Esto significa que las variables se modelan multiplicadas entre sí por ejemplo.</p>
</li>
</ul>
<div class="arithmatex">\[ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 +  \beta_3X_1X_2 + \epsilon \]</div>
<ul>
<li>Podemos aproximar relaciones no lineales extendiendo el modelo a regresión polinómica.
Suponiendo que los datos provienen de un modelo polinómico, podemos ver en el siguiente gráfico cómo cambia al agregar el término de no linealidad.
La línea naranja es la regresión lineal simple y la celeste (que ajusta casi perfecto) es la regresión polinómica que respeta el proceso generador de los datos (la ecuación que se ve en el gráfico). Vemos que la variable Y depende de X linealmente pero también de X al cuadrado, lo que le da la curvatura.</li>
</ul>
<p><img alt="Image" src="../../img/2019-05-15-regresion-lineal-islr-reg2-1.png" /></p>
<h4 id="potenciales-problemas"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#potenciales-problemas">Potenciales problemas</a></h4>
<h5 id="no-linealidad-de-los-datos"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#no-linealidad-de-los-datos">No linealidad de los datos</a></h5>
<p>Si la relación entre nuestras variables independientes y la dependiente no es lineal nuestro modelo va a tener sesgo alto. Para el modelo simple es fácil de ver al graficar X vs Y en un gráfico de puntos pero con muchas variables eso ya no es tan sencillo. Un buen enfoque es realizar una regresión lineal y graficar los residuos contra los valores predichos.</p>
<p>Es un caso un poco extremo pero supongamos que la relación es no lineal, polinómica  de orden 2 (como el ejemplo de arriba).<br />
Si nosotros corremos una regresión lineal, nuestros residuos van a seguir un patrón muy obvio.
<img alt="Image" src="../../img/2019-05-15-regresion-lineal-islr-unnamed-chunk-2-1.png" /></p>
<p>Claramente en ese gráfico el residuo no está centrado en 0...
En cambio, si nosotros corremos una regresión lineal para un modelo realmente lineal, o para este caso, corremos un modelo no lineal, deberíamos ver una nube de puntos dispersa para los resiudos, centrada en 0 y con algún desvío estándar.
Idealmente veríamos algo asi.</p>
<p><img alt="Image" src="../../img/2019-05-15-regresion-lineal-islr-unnamed-chunk-3-1.png" /></p>
<h5 id="correlacion-de-los-terminos-de-error"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#correlacion-de-los-terminos-de-error">Correlación de los términos de error</a></h5>
<p>Uno de los supuestos de la regresión lineal es que los errores no están correlacionados, es decir que el error <span class="arithmatex">\(\epsilon_i\)</span> de una observación no nos aporta información acerca del error <span class="arithmatex">\(\epsilon_j\)</span> de otra observación. Son independientes.<br />
Si esto no se cumple lo que sucede es que el SE de los coeficientes estimados es menor al real y puede llevarnos a confiar más en nuestro modelo de lo que deberíamos.<br />
Los errores correlacionados suelen suceder más frecuentemente en series de tiempo pero también pueden darse  en estudios experimentales mal diseñados.</p>
<h5 id="varianza-de-los-terminos-de-error-no-constante-heterocedasticidad"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#varianza-de-los-terminos-de-error-no-constante-heterocedasticidad">Varianza de los términos de error no constante. (Heterocedasticidad)</a></h5>
<p>Otro supuesto de la regresión lineal es que la varianza de los errores es constante <span class="arithmatex">\(Var(\epsilon_i) = \sigma^2\)</span>. Esto no siempre es el caso.
En este ejemplo vemos como los resiudos siguen centrados en 0 pero con una dispersión mucho mayor a medida que avanzamos en el eje X.
<img alt="Image" src="../../img/2019-05-15-regresion-lineal-islr-unnamed-chunk-4-1.png" />
Entre las soluciones para este problema se encuentra transformar la variable dependiente - <span class="arithmatex">\(ln(y)\)</span> por ejemplo, o utilizar Mínimos cuadrados ponderados, que pondera por la inversa de la varianza. El libro no se explaya mucho más al respecto en este capítulo.</p>
<h5 id="outliers"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#outliers">Outliers</a></h5>
<p>Los outliers son observaciones cuya variable dependiente tienen valores que se alejan mucho del patrón regular de los datos, por ejemplo debido a un error de medición o problema al registrar la información. Los outliers pueden tener diversas consecuencias en los modelos lineales. Puede afectar la estimación de los parámetros, puede afectar el ajuste del modelo (caída del <span class="arithmatex">\(R^2\)</span>) o puede por ejemplo aumentar los intervalos de confianza ya que el outlier afecta el RSE que es común a todos los intervalos. Todo esto puede ser generado por una sola observación. Generalmente si no se distinguen en el análisis exploratorio pueden saltar a la vista analizando los resiudos del modelo (o los residuos estandarizados).</p>
<h5 id="puntos-con-alto-leverage"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#puntos-con-alto-leverage">Puntos con alto "leverage"</a></h5>
<ul>
<li>Si alguno tiene una traducción satisfactoria bienvenido sea..
Los puntos con alto leverage son aquellos cuyo valor de la variable independiente se aleja del rango estándar. Las observaciones con esta característica tienden a afectar en buena medida a la curva ajustada y por ende a los parámetros de nuestro modelo. Nuestra estimación por mínimos cuadrados puede verse muy influenciada por estos puntos e invalidar el ajuste por eso es muy necesario identificar estas observaciones.<br />
En regresión simple es sencillo de ver porque resaltan si graficamos una nube de puntos pero en regresión múltiple es más difícil de ver ya que debemos encontrar anomalías en el conjunto de todas las variables. Es decir que una observación puede estar en el rango individual de cada variable pero si miramos a nivel conjunto, esa combinacion dentro de los rangos individuales es súper anómala. Con más de dos variables independientes se dificulta identificar visualmente. Para ayudar en estos casos se puede calcular el <em>estádistico de leverage</em> en algún programa estadístico.</li>
</ul>
<h5 id="colinealidad"><a class="toclink" href="../../2019/05/25/regresion-lineal-islr/#colinealidad">Colinealidad</a></h5>
<p>Este problema refiere a la alta correlación entre variables independientes del modelo, es decir que tienden a aumentar o decrecer de manera conjunta. Esto genera que sea difícil (o imposible en el extremo) diferenciar el impacto de cada una de ellas en la variable dependiente.<br />
En una regresión lineal esto se traduce en aumento de la varianza de los estimadores y por ende incertidumbre sobre los parámetros estimados. A modo intuitivo, con variables con alta correlación puede haber una gran cantidad de combinaciones de coeficientes para estas variables que resulten en un mismo ajuste (<span class="arithmatex">\(R^2\)</span>) y por ende mínimos cuadrados es <em>indistinto</em> frente a ellos. Cambiando alguna observación puede que el modelo pase de una combinación a otra muy disinta en ese arco de posibilidades. Otra consecuencia es que el aumento de la varianza de los coeficientes reduce el estadístico t que miramos para la significatividad y puede que lleve a no rechazar una hipótesis nula que debía ser rechazada. La potencia del test de hipótesis se ve disminuida por la colinealidad.<br />
No solo la correlación sirve para detectar colinealidad. Puede existir multicolinealidad en donde varias variables son colineales aún sin tener alta correlación de a pares. Posiblemente se deba a combinación lineal generada por algunas de las variables. Para estos casos lo que se puede mirar es el VIF ( Variance Inflation Factor) en inglés. Este estadístico se calcula para cara variable y compara la varianza del estimador al tener la variable en el modelo versus ajustando un modelo solo con esa variable. Cuanto mayor es el VIF, mayores problemas de colinealidad resalta.
Se puede calcular con la siguiente formula donde <span class="arithmatex">\(R^2_{X_j|X_{-j}}\)</span> es el <span class="arithmatex">\(R^2\)</span> de la regresión de <span class="arithmatex">\(X_j\)</span> contra todas las otras variables independientes del modelo.
$$ VIF(\hat \beta_j) = \frac{1}{1 - R^2_{X_j|X_{-j}}}$$
La solución a este problema suele ser descartar alguna de las variables o agruparlas de alguna manera para quedarnos con una única variable que represente a ambas.</p>

    <nav class="md-post__action">
      <a href="../../2019/05/25/regresion-lineal-islr/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2019-05-06 00:00:00">2019-05-06</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../../category/estadistica/" class="md-meta__link">estadistica</a></li>
        
        
          
          <li class="md-meta__item">
            
              8 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="aprendizaje-estadistico-islr-capitulo-2"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/">Aprendizaje Estadístico - ISLR Capitulo 2</a></h2>
<p>Suponemos que las variables que encontramos en un set de datos son generadas a través de un proceso generador de datos (DGP por sus siglas en inglés) cuya expresión es: 
$$ Y = f(X) + \epsilon $$</p>
<p>Donde Y es la variable, en este caso la dependiente o la que queremos explicar. f(X) es una función respecto a otra/s variable/s X (independientes) y <span class="arithmatex">\(\epsilon\)</span> es el error irreducible, es decir un valor aleatorio con media 0 pero que no depende de otras variables, es al azar. Puede referir a errores de medición, cambios inmesurables en las situaciones del experimento o simplemente azar en la generación real de los datos. Cabe destacar que f(X) es desconocida para nosotros y justamente lo que queremos explorar con el análisis estadístico. Puede tenerse suposiciones o conocimiento de la forma funcional (lineal, no lineal, etc) pero en principio no tenemos mayores certezas y esperamos aprender a partir de la muestra que analizamos.</p>
<h4 id="por-que-estimar-fx"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#por-que-estimar-fx">Por qué estimar f(X)?</a></h4>
<p>Los dos principales motivos para interesarse en f(X) son <em>predicción</em> (de Y) e <em>inferencia</em> de los parámetros de f(X).</p>
<h5 id="prediccion"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#prediccion">Predicción</a></h5>
<p>Queremos predecir valores de Y para nuevos datos X. Como <span class="arithmatex">\(\epsilon\)</span> en promedio es 0 podemos aproximar Y de la forma: 
$$ \hat Y = \hat f(X)$$
La precisión de <span class="arithmatex">\(\hat Y\)</span> va a depender del <strong>error reducible</strong> y del <strong>error irreducible</strong>. El primero depende de qué tan bien nos aproximemos a la verdadera f(X) y puede ser potencialmente reducido si utilizamos las técnicas más adecuadas para el caso. El segundo error es justamente irreducible y es porque nuestra aproximación no puede tener en cuenta a <span class="arithmatex">\(\epsilon\)</span>. El término aleatorio introducido por esa variable no lo podemos estimar para cada observación y por lo tanto debemos convivir con ese margen de error.</p>
<p>Suponiendo que tenemos una estimación <span class="arithmatex">\(\hat f\)</span> y un set de datos X puede probarse que:
$$ E(Y - \hat Y)^2 = E[f(X) + \epsilon - \hat f(X)]^2$$
                   $$   \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \     = \underbrace{[f(X) - \hat f(X)]^2}<em>\text{Reducible} + \underbrace{Var(\epsilon)}</em>\text{Irreducible}$$</p>
<p>Donde <span class="arithmatex">\(E(Y-\hat Y)^2\)</span> es el promedio o valor esperado de la diferencia al cuadrado del valor real de Y y de la predicción correspondiente.
<span class="arithmatex">\(Var(\epsilon)\)</span> es la varianza del término de error <span class="arithmatex">\(\epsilon\)</span>.</p>
<h5 id="inferencia"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#inferencia">Inferencia</a></h5>
<p>Este enfoque se basa en entender la relación entre las variables de X y la dependiente. Es necesario entender bien la f(X) elegida para poder interpretar sus coeficientes y poder ver qué variables están asociadas con Y, cómo es esa relación, cuál es la forma de la función f(X), etc para poder actuar sobre las variables X o comprender su efecto aunque no siendo tan exigentes con el poder de predicción de nuestro modelo.</p>
<h4 id="como-estimar-fx"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#como-estimar-fx">Como Estimar f(X)?</a></h4>
<h5 id="metodos-parametricos"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#metodos-parametricos">Métodos Paramétricos</a></h5>
<p>Los métodos paramétricos se conforman por dos etapas.<br />
La primera es asumir o suponer la forma funcional de f(X). Podes definir por ejemplo que f(X) es una función lineal de la forma
$$ f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p$$  </p>
<p>Una vez definida la forma del modelo la segunda etapa consiste en estimar los parámetros con algún método, a partir de los datos de entrenamiento. En este caso sería estimar todos los <span class="arithmatex">\(\beta\)</span>. Por ejemplo para las funciones lineales se suele utilizar el método de <em>mínimos cuadrados ordinario</em>.<br />
La ventaja de definir una forma funcional es que luego es más sencillo estimar sus parámetro y el problema se reduce a eso finalmente. Por el otro lado, posiblemente la forma que elijamos no sea exactamente igual a la real (DGP) y tengamos que aceptar que va a haber errores debido a eso. Si estamos muy lejos de la forma real esos errores serán groseros. Existen modelos flexibles que permiten ajustar modelos con diferentes formas de f(X) pero en general requieren estimar más parámetros  y son más propensos a sufrir sobreajuste/overfitting que básicamente es ajustarse mucho al ruido o error (<span class="arithmatex">\(\epsilon\)</span>) en los datos de entrenamiento y luego ajustar mal en testeo.  </p>
<h5 id="metodos-no-parametricos"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#metodos-no-parametricos">Métodos No Paramétricos</a></h5>
<p>Los métodos no paramétricos no requieren definir explícitamente una forma funcional de f. Buscan un f que sea lo más cercano posible a los datos sin ser demasiado estricto o flexible. Al no asumir una forma puede cubrir potencialmente un rango mucho mayor. El problema es que al no reducir el problema a estimar parámetros necesitan muchas más observaciones para estimar f de forma medianamente precisa. En general uno tiene que decidir el nivel de "suavidad" del modelo, lo cual afecta que tan variable termina siendo la estimación. Sirve para encontrar el punto de fleixibilidad/rigidez del modelo que queremos para que no sobreajuste (ni falle demasiado).</p>
<h4 id="prediccion-vs-interpretabilidad"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#prediccion-vs-interpretabilidad">Predicción vs Interpretabilidad</a></h4>
<p>Uno puede elegir entre modelos flexibles o más rígidos. Con pocas observaciones a veces uno no puede alejarse mucho de los rígidos pero suponiendo que uno tiene muchos datos, a veces puede igualmente elegir rígidez frente a modelos flexibles que permiten ajustar varias formas de f. 
El motivo es que generalmente los modelos restrictivos son más fáciles de interpretar y se le puede dar un significado claro a sus coeficientes mientras que con formas muy flexibles no es sencillo entender el impacto de las variables de manera individual. La elección va a depender del objetivo del análisis y de qué tan bien o mal nuestros modelos ajustan a los datos.</p>
<h4 id="modelos-supervisados-vs-no-supervisados"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#modelos-supervisados-vs-no-supervisados">Modelos Supervisados vs No Supervisados</a></h4>
<p>Nuestros datos pueden tener una variable dependiente que queremos explicar o predecir en base a un set de variables independientes, con algún modelo a definir. Los casos de este estilo son llamados <em>supervisados</em> porque sabemos la "respuesta" (nuestra variable Y) y podemos validar nuestros modelos contra la realidad.<br />
Si los datos no tienen una variable dependiente lo que se puede hacer es un análisis no supervisado donde por ejemplo lo que se puede hacer es agrupar las observaciones en clusters o grupos. Es decir segmentar en distintas clasificaciones y descubrir patrones. El desafío es que no hay en los datos nada contra qué validarlo, aunque sí contra el conocimiento del dominio o de la temática.</p>
<h4 id="regresion-vs-clasificacion"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#regresion-vs-clasificacion">Regresión vs Clasificación</a></h4>
<p>En los modelos supervisados, nuestra variable dependiente puede ser <em>cuantitativa</em> o <em>cualitativa</em>.<br />
En el primer caso la variable toma valores númericos, como por ejemplo la altura de una persona, el precio de una propiedad, etc. Son problemas de regresión.  <br />
En el segundo caso la variable dependiente puede tomar el valor de una clase o categoría. Por ejemplo, género de una persona, si paga o no paga su deuda, etc. Son problemas de clasificación.</p>
<h4 id="midiendo-el-ajuste-del-modelo"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#midiendo-el-ajuste-del-modelo">Midiendo el ajuste del modelo</a></h4>
<h5 id="regresion"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#regresion">Regresión</a></h5>
<p>Para problemas de regresión una de las medidas más utilizadas es el Error Cuadrático Medio (MSE por sus siglas en inglés).
$$ MSE = \frac{1}{n} \sum_{i=1}^n(y_i - \hat f(x_i))^2 $$</p>
<p>Es básicamente la diferencia promedio entre la realidad y lo que predice nuestro modelo elevado al cuadrado. Esto último es para que los errores sean siempre positivos aunque subestimemos o sobreestimemos (y por su comodidad para cálculos matemáticos).</p>
<p>En primero lugar se calcula este valor con los datos de entrenamiento sin embargo lo que realmente importa es como performa el modelo en datos de testeo, es decir en datos que no fueron utilizados para estimar f(X). Podemos decir que cada modelo debería tener un MSE de entrenamiento y un MSE de testeo. Debido a la posibilidad de sobreajuste y a las diferencias en muestras nada garantiza que el modelo que estimemos con menor MSE en entrenamiento también sea el de menor MSE en testeo. </p>
<p>A medida que aumentamos la flexibilidad de un modelo (sus grados de libertad) el MSE en entrenamiento va a disminuir, ya que tiene más herramientas para ajustarse a los datos pero puede que sobreajuste y por lo tanto no se traduzca en un menor MSE en testeo.</p>
<h4 id="el-tradeoff-entre-sesgo-y-varianza"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#el-tradeoff-entre-sesgo-y-varianza">El tradeoff entre Sesgo y Varianza</a></h4>
<p>No está demostrado en el libro pero es posible descomponer el MSE esperado de una observación de testeo en sesgo de <span class="arithmatex">\(\hat f(x_0)\)</span>, varianza de <span class="arithmatex">\(\hat f(x_0)\)</span> y varianza del error irreducible <span class="arithmatex">\(\epsilon\)</span>.</p>
<div class="arithmatex">\[ E(y_0 - \hat f(x_0))^2 = Var(\hat f(x_0)) + [Sesgo(\hat f(x_0))]^2 + Var(\epsilon)\]</div>
<p>El lado izquierdo de la ecuación es el MSE esperado y coresponde MSE de testeo promedio que obtendríamos si estimaramos f utilizando una gran cantidad de sets de entrenamiento y testearamos cada uno en <span class="arithmatex">\(x_0\)</span>.</p>
<p>Algunas observaciones:</p>
<ul>
<li>El MSE nunca puede ser menor que la varianza de <span class="arithmatex">\(\epsilon\)</span>. Es un término fijo y por eso se lo llama error irreducible.</li>
<li>La varianza es cuanto cambiar <span class="arithmatex">\(\hat f\)</span> si utilizamos otro set de entrenamiento. Siempre va a cambiar con otro set pero idealmente ese cambio no debería ser grande. Modelos muy flexibles tienden a cambiar más frente a distintos sets y son más inestables.</li>
<li>Sesgo es el error provocado por la diferencia entre el modelo elegido y el verdadero proceso generador de los datos. En general modelos más flexibles tienen menor sesgo ya que pueden ajustar mayor variedad de formas funcionales.</li>
<li>Al aumentar la flexibilidad de un modelo en general reducimos el sesgo pero aumentamos la varianza. En general en un primer momento el sesgo suele disminuir a mayor velocidad de lo que aumenta la varianza y por lo tanto el MSE esperado se reduce. Sin embargo llega un punto donde mayor flexibilidad reduce menos el sesgo que lo que aumenta la varianza y el MSE empieza a aumentar. Es el primer indicio de sobreajuste. Por eso se habla de tradeoff o "balance".</li>
<li>En la realidad donde la verdadera f del DGP es inobservable no suele ser posible calcular explícitamente el MSE de testeo, el sesgo o la varianza de un método estadístico pero el proceso de fondo aplica y siempre debemos tener en mente el tradeoff.</li>
</ul>
<h5 id="clasificacion"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#clasificacion">Clasificación</a></h5>
<p>Para problemas de clasificación uno de los enfoques más frecuentes para cuantificar la precisión de una función estimada <span class="arithmatex">\(\hat f\)</span> se suele usar el porcentaje de error en los datos de entrenamiento.</p>
<p>$$ \frac{1}{n} \sum_{i=1}^nI(y_i \neq \hat y_i) $$
Básicamente es el porcentaje de observaciones clasificadas erroneamente.
Al igual que con MSE es de gran importance el porcentaje de error en los datos de testeo.</p>
<h5 id="clasificador-de-bayes"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#clasificador-de-bayes">Clasificador de Bayes</a></h5>
<p>No lo demuestra en el libro pero la mejor manera de reducir el porcentaje de error en test es asignar a cada observación la clase con mayor probabilidad (según el DGP ). Es un concepto muy sencillo, dado X, asignar la clase cuya chance de acierto sea mayor.</p>
<p>$$ Pr(Y = j | X = x_0) $$
El porcentaje de error de Bayes (es decir el error luego de clasificar siguiendo esa regla) es análogo al Error Irreducible de regresión.
Hay que tener en cuenta que la distribución condicional de Y dado X no lo sabemos en los casos aplicados en la vida real, sería como saber la función f(X) o el DGP y por lo tanto no lo podemos calcular.</p>
<h5 id="k-vecinos-mas-cercanos-knn-en-ingles"><a class="toclink" href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/#k-vecinos-mas-cercanos-knn-en-ingles">K vecinos más cercanos (KNN en inglés)</a></h5>
<p>Idealmente uno querría aplicar el clasificador de Bayes pero es imposible ya que no sabemos la distribución real de los datos (es justamente lo que queremos estimar). KNN intenta aproximarse a la distribución condicional para clasificar las observaciones. Lo que hace este método es, dado un valor de K que elegimos nosotros, clasificar cada nueva observación según la clase mayoritaria entre las K observaciones más cercanas a esta.</p>
<p>$$ Pr(Y = j | X = x_0) = \frac{1}{K} \sum_{i \in N_0} I(y_i = j) $$
El valor que seleccionemos de K afecta en gran medida las predicciones del modelo. Un K menor hace más variable el modelo ya que selecciona menos observaciones y por lo tanto pocos cambios en el set de entrenamiento cambian la clasificación. Suele reducir el sesgo pero ser mas variable. Es análogo a hacer más flexible un modelo en regresión. Valores de K más grandes seleccionan puntos en un entorno más abarcativo y por lo tanto suele ser más constante pero con sesgo superior.<br />
Al igual que en regresión hay que tener cuidado con el sobreajuste. Reducir K garantiza menos errores en los datos de entrenamiento pero pasado un umbral la varianza aumenta en mayor medida y el porcentaje de error en test se incrementa.</p>
<p><strong>Conclusión</strong>: Tanto en Clasificación como en Regresión la elección del nivel de flexibilidad  es central en el éxito de método de aprendizaje estadístico.</p>

    <nav class="md-post__action">
      <a href="../../2019/05/06/aprendizaje-estad%C3%ADstico-islr-capitulo-2/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2019-04-20 00:00:00">2019-04-20</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../../category/matematica/" class="md-meta__link">matematica</a>, 
              <a href="../../category/algebra/" class="md-meta__link">algebra</a></li>
        
        
          
          <li class="md-meta__item">
            
              7 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="esencia-del-algebra-lineal"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/">Esencia del Algebra Lineal</a></h2>
<p>El álgebra lineal está por todas partes en estadística y data science. Matrices, vectores y transformaciones son términos que se escuchan seguido y están detrás de muchos de los métodos y algoritmos que se usan hoy por hoy. Aunque no sea necesario saber del tema para correr un modelo empaquetado en una librería de R, es muy útil entender lo que hacemos realmente ya que nos permite ver a los modelos como algo lógico y no una caja negra mágica.</p>
<p>Hay una serie de videos excelente en inglés que mediante visualizaciones y animaciones permite entender la intuición de muchos de los conceptos básicos, que solo con un libro puede ser medio críptico o poco imaginable.
Para el que le interese: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">ESSENCE OF LINEAR ALGEBRA</a> por 3Blue1Brown.</p>
<p>Este post, aunque quizás medio desordenado y sin mucha prolijidad, es una recopilación de algunas notas. Puede que queden términos en inglés intercalados.</p>
<h3 id="matrices-y-vectores"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#matrices-y-vectores">Matrices y vectores</a></h3>
<ul>
<li>Vector vive en <em>n</em> dimensiones.</li>
<li>Suma de vectores es combinación lineal</li>
<li>En <span class="arithmatex">\(R^{2}\)</span> <span class="arithmatex">\(\hat \imath = \left&lt; 1, 0 \right&gt; \text{y} \hat \jmath = \left&lt; 0, 1 \right&gt;\)</span> forman una base. Cualquier punto es una combinación lineal de ellos.     </li>
<li>Span es el espacio que pueden generar x vectores. <span class="arithmatex">\(R^{2}\)</span> es el span de  <span class="arithmatex">\(\left&lt; 1, 0 \right&gt; \left&lt; 0, 1 \right&gt;\)</span></li>
<li>Vector puede ser pensado como una flecha desde el origen (0,0) a las coordenadas que lo identifican. O como un punto directo en las coordenadas..</li>
<li>Matriz es una transformación. Lleva un vector a otro punto. Si transformamos cada posible vector de un espacio por la matriz podemos ver como el espacio es transformado. Ej: rotar, invertir, estirar.</li>
<li>Si transformamos una base, cada punto nuevo puede generarse transformando la nueva base. <br />
Por ej: <span class="arithmatex">\(z = \left&lt; 3, 2 \right&gt; \text{es } 3\begin{bmatrix} 1 \\ 0 \end{bmatrix} + 2\begin{bmatrix} 0 \\ 1 \end{bmatrix} = 3\hat \imath + 2\hat \jmath\)</span><br />
Aplicando la transformación de la matriz A = <span class="arithmatex">\(\begin{pmatrix} A &amp; B \\ C &amp; D \end{pmatrix} \text{obtenemos los nuevos vectores base } \hat \imath^{*} \text{y } \hat \jmath^{*}\)</span><br />
<span class="arithmatex">\(z^{*} = 3\hat \imath^{*} + 2\hat \jmath^{*}\)</span> </li>
<li>Multiplicar 2 matrices es transformar un espacio con la primera matriz ( desde la derecha) y luego transformar el resultado por la segunda matriz. Ej: Rotar un espacio y luego invertirlo.</li>
<li>AB != BA -&gt; El orden de las transformaciones importa y se lee de derecha a izquierda.</li>
<li>
<p>La matriz (transformación) ya dice como van a ser las nuevas bases.<br />
Si la matriz es <span class="arithmatex">\(\begin{pmatrix} A &amp; B \\ C &amp; D \end{pmatrix}\)</span>, el nuevo <span class="arithmatex">\(\hat \imath^{*}\)</span> es <span class="arithmatex">\(\begin{bmatrix} A \\ C \end{bmatrix}\)</span> y <span class="arithmatex">\(\hat \jmath^{*}\)</span> es <span class="arithmatex">\(\begin{bmatrix} B \\ D \end{bmatrix}\)</span><br />
Ej: <span class="arithmatex">\(z = \left&lt; 3, 2 \right&gt;  z^{*} =  \begin{bmatrix} A &amp; B \\ C &amp; D \end{bmatrix}\begin{bmatrix} 3 \\ 2 \end{bmatrix} = \begin{bmatrix} 3A + 2B \\ 3C + 2D \end{bmatrix}\)</span><br />
Se puede ver también como:  <span class="arithmatex">\(<span class="arithmatex">\(z = 3\hat \imath + 2\hat \jmath \text{  }  z^{*} = 3\hat \imath^{*} + 2\hat \jmath^{*} = 3\begin{bmatrix} A \\ C \end{bmatrix} + 2\begin{bmatrix} B \\ D \end{bmatrix} = \begin{bmatrix} 3A + 2B \\ 3C + 2D \end{bmatrix}\)</span>\)</span>
&nbsp;</p>
</li>
<li>
<p>!!!. Las transformaciones afectan el area (en R2, el volumen en R3..) de las figuras en el espacio (todas por igual). El <em>DETERMINANTE</em> de una matriz mide ese cambio.</p>
</li>
<li>Si el determinante <strong>es 0</strong> significa que se perdió una dimensión o que todo se comprimió. Pasa de <span class="arithmatex">\(R^{2}\)</span> a una recta (o a un punto!)</li>
<li>Si el determinante <strong> es &lt; 0</strong> significa que el espacio se invirtió (en sentido.. como dar vuelta una hoja) pero |DET| siguen siendo el cambio en el area.</li>
<li>A^-1^A = I -&gt; una transformación que no hace nada.</li>
<li>
<p>Si DET(A) = 0 no existe la matriz inversa. Ej. <span class="arithmatex">\(R^{2}\)</span> -&gt; si det(A) = 0 la transformación lleva el espacio a una recta. No hay función que lleve cada vector de la recta a un punto en <span class="arithmatex">\(R{2}\)</span>. No hay vuelta atrás.
&nbsp;
&nbsp;</p>
</li>
<li>
<p>Si una transformación lleva todos los puntos a una recta tiene rango 1, si lleva a un plano rango 2, y así.. <strong>RANGO</strong> es el número de dimensiones del output. Rango completo es cuando mantiene las dimensiones del input.</p>
</li>
<li>El conjunto de posibles outputs de <span class="arithmatex">\(A\vec v\)</span> es el Column Space = Span de las columnas</li>
<li>Cuando perdés dimensiones por la transformación todo un conjunto de vectores pasa a ser (0,0). Eso se llama <strong>Null Space</strong> o <strong>Kernel</strong></li>
<li>Matrices no cuadradas cambian la dimensión del espacio.<br />
$$ \begin{bmatrix} A &amp; D \ B &amp; E \ C &amp; F \end{bmatrix} \begin{bmatrix} 1 \ 1 \end{bmatrix} = \begin{bmatrix} A + D \ B + E \ C + F \end{bmatrix} $$ 
Quedan todos los puntos de <span class="arithmatex">\(R^{2}\)</span> en un plano en el espacio <span class="arithmatex">\(R^{3}\)</span>. De acá viene la restricción para multiplicar matrices. La cantidad de columnas de la transformación tiene que ser igual a la dimensión del input</li>
</ul>
<h4 id="dot-product-o-producto-interno"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#dot-product-o-producto-interno">DOT PRODUCT o PRODUCTO INTERNO</a></h4>
<ul>
<li>Dot product entre dos vectores equivale a proyectar uno en el otro y multiplicar sus largos. <span class="arithmatex">\(\vec A \cdot \vec B = |A^{*}| * |B|\)</span><br />
<span class="arithmatex">\(A^{*}\)</span> es el vector A proyectado en B.</li>
<li><span class="arithmatex">\(\vec B\)</span> es un vector 2D pero también se lo puede ver como una matriz 1x2 que lleva del 2D a la recta.<br />
<span class="arithmatex">\(\vec B \cdot \vec A = B \vec A \text{que sería llevar A al espacio transformado por B.}\)</span><br />
<span class="arithmatex">\(B = \begin{bmatrix} B_x &amp; B_y \end{bmatrix}\)</span> tiene en sus columnas donde queda <span class="arithmatex">\(\hat \imath \text{y } \hat \jmath\)</span> (los vectores unitarios) al ser transformados o algun valor escalado de esto.<br />
<span class="arithmatex">\(\vec A \cdot \vec B\)</span> es el valor de A en la recta a la que te lleva la transformación B.</li>
<li>Es equivalente proyecto B en A.</li>
<li>Si Dot Product &gt; 0, tienen dirección similar.</li>
<li>Si Dot Product = 0, son ortogonales - proyección que cae en el origen.</li>
<li>Si Dot Product &lt; 0, tienen direcciones opuestas.</li>
</ul>
<p>&nbsp;
&nbsp;</p>
<h4 id="cross-product"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#cross-product">CROSS PRODUCT</a></h4>
<ul>
<li>Está definido para vectores en <span class="arithmatex">\(R^{3}\)</span></li>
<li>El cross product <span class="arithmatex">\(\vec u \times \vec v\)</span> es el area del paralelograma que se puede imaginar con las paralelas de los vectores (imaginandolo en <span class="arithmatex">\(R^{2}\)</span>. El signo depende de la orientación de los vectores. El vector de la "derecha" tiene que estar primero para que el cross product sea &gt; 0.</li>
<li>En realidad el paralelograma formado por dos vectores en R^3^ tiene area equivalente al <strong>Largo</strong> del vector output de su cross product. Ese vector es ortogonal al paralelograma.</li>
</ul>
<p>&nbsp;
&nbsp;</p>
<h4 id="cambio-de-base"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#cambio-de-base">CAMBIO DE BASE</a></h4>
<ul>
<li>Distintos sistemas de coordenadas definen <span class="arithmatex">\(\hat \imath = \left&lt; 1, 0 \right&gt;, \hat \jmath \left&lt; 0, 1 \right&gt;\)</span> como algo distinto. <strong>NO</strong> hay una sola "grilla" válida. El espacio no tiene grilla predeterminada.</li>
<li>Un mismo vector tiene distintas coordenadas según el sistema de bases desde donde se lo mire.</li>
<li>Para pasar de una base a otra se aplica una transformación lineal.<br />
Si <span class="arithmatex">\(\vec v\)</span> es un vector que queremos pasar de una base a otra, lo transformamos por la nueva base.<br />
Y <span class="arithmatex">\(\hat \imath^{*} =  \left&lt; \hat \imath^{*}_1, \hat \imath^{*}_2 \right&gt;, \hat \jmath^{*} =  \left&lt; \hat \jmath^{*}_1, \hat \jmath^{*}_2 \right&gt;\)</span>
Entonces:
<span class="arithmatex">\(<span class="arithmatex">\(\begin{bmatrix} \hat \imath^{*}_1 &amp; \hat \jmath^{*}_1 \\ \hat \imath^{*}_2 &amp; \hat \jmath^{*}_2 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} v^{*}_1 \\ v^{*}_2 \end{bmatrix}\)</span>\)</span><br />
Donde <span class="arithmatex">\(\begin{bmatrix} v^{*}_1 \\ v^{*}_2 \end{bmatrix}\)</span> es el vector en la nueva base, es decir, serían las coordenadas del vector <span class="arithmatex">\(\vec v\)</span> en el nuevo sistema de coordenadas y representando ese punto bajo el sistema de coordenadas original. -&gt; Como se vería <span class="arithmatex">\(\vec v\)</span> en la nueva base? desde un punto de vista de la base original.<br />
La matriz transforma un vector siguiendo en el lenguaje de la base original.<br />
Ej: Si <span class="arithmatex">\(\vec v\)</span> es (1,2) en el sistema cartesiano típico y aplicamos la matriz de cambio de base, un vector (1,2) bajo otros ejes se ubicaría en otro punto del espacio. Qué punto es ese bajo el sistema cartesiano? Es (1,2) en el nuevo, pero queremos saber su equivalente en el sistema original.</li>
<li>Por otra parte si queremos saber que coordenadas tomaría el vector <span class="arithmatex">\(\vec v\)</span> bajo otra base debemos multiplicar por la inversa de la transformación. Transforma el vector al lenguaje de la nueva base.
Responde a la pregunta. Qué coordenadas toma el punto V_1, V_2 del espacio en el sistema nuevo?
&nbsp;</li>
<li>Para aplicar una transformación a otra base conviene llevar el vector a transformar a la base original, transformar y reconvertir a la nueva base.
$$ [A]^{-1}[T][A]\vec v = \vec v^{*}$$<br />
A lo expresa en términos de la base original, luego se le aplica la transformación T y luego se lo devuelve al lenguaje de la nueva transformación.</li>
</ul>
<p>&nbsp;
&nbsp;</p>
<h4 id="eigenvalues-y-eigenvectors-autovalores-y-autovectores"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#eigenvalues-y-eigenvectors-autovalores-y-autovectores">Eigenvalues y Eigenvectors (autovalores y autovectores)</a></h4>
<ul>
<li>!!! Al aplicar una transformación lineal a un espacio algunos vectores no cambian de dirección, solo se estiran o contraen pero sobre la misma recta. El resto sí se mueve. Los que se mantienen son los eigenvectors, y su factor de expansión o contracción es su eigenvalue.<br />
Si A es la matriz de transformación, <span class="arithmatex">\(\vec v\)</span> es un eigenvector y <span class="arithmatex">\(\lambda\)</span> su eigenvalue.
$$ A\vec v = \lambda \vec v$$  </li>
<li>Si una transformación es una matriz diagonal, lo único que hace es estirar <span class="arithmatex">\(\hat \imath \text{y } \hat \jmath\)</span> por lo tanto los vectores base son eigenvectors y la diagonal son los eigenvalues.</li>
<li>Si cambiamos la base a una formada por los eigenvectors (que spanean el espacio) de la matriz podemos aplicar la transformación (la matriz original de donde salieron los eigenvectors) a esta nueva base y solo la va a estirar, por lo tanto es una transformación diagonal, que permite calculos mucho más fácil. Después habría que volver a la base original.<br />
A -&gt; Matriz de transformación
E -&gt; Matriz de autovectores que forman la nueva base <span class="arithmatex">\(\begin{bmatrix} e_11 &amp; e_21 \\ e_12 &amp; e_22 \end{bmatrix}\)</span>
D -&gt; Matriz Diagonal cuyos valores son los eigenvalues
$$ E^{-1}AE=D$$</li>
</ul>
<p>E cambia la base a eigenvectors (expresado en la base original), A aplica transformación y E^-1^ lo lleva al lenguaje de la nueva base (queda expresado en las nuevas coordenadas)
&nbsp;
&nbsp;</p>
<h4 id="espacios-vectoriales-abstractos"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#espacios-vectoriales-abstractos">Espacios Vectoriales Abstractos</a></h4>
<ul>
<li>!!! Ver <em>funciones</em> como un tipo especial de vectores.</li>
<li>Las funciones se pueden sumar y escalar <span class="arithmatex">\(f(x) + g(x) \text{y } 2f(x)\)</span></li>
<li>Existen transformaciones lineales de funciones, convierten una función en otra. También conocidas como "operadores"</li>
<li>Para que una transformación sea lineal tiene que cumplir aditividad y mulitplicación por escalar<br />
$$ L(\vec v + \vec  w) = L(\vec v) + L(\vec w)$$
$$ L(c\vec v) = cL(\vec v)$$</li>
<li>En general cualquier espacio que cumpla los axiomas los espacios vectoriales puede ser considerado uno.</li>
</ul>

    <nav class="md-post__action">
      <a href="../../2019/04/20/esencia-del-algebra-lineal/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2019-04-03 00:00:00">2019-04-03</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../../category/estadistica/" class="md-meta__link">estadistica</a>, 
              <a href="../../category/r/" class="md-meta__link">R</a></li>
        
        
          
          <li class="md-meta__item">
            
              4 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="funciones-de-probabilidad-y-distribucion"><a class="toclink" href="../../2019/04/03/funciones-de-probabilidad-y-distribucion/">Funciones de Probabilidad y Distribucion</a></h2>
<h3 id="variables-aleatorias"><a class="toclink" href="../../2019/04/03/funciones-de-probabilidad-y-distribucion/#variables-aleatorias">Variables Aleatorias</a></h3>
<p>Consideremos un experimento cuyo espacio muestral denominaremos <code>S</code>. <br />
Una funcion valuada en el dominio de los reales definida en <code>S</code> es una variable aleatoria. </p>
<p>En otras palabras es una función que asigna a cada resultado posible de un experimento un valor real.</p>
<p>Por ejemplo:</p>
<blockquote>
<p>Si el experimento es lanzar una moneda 10 veces hay 2^10^ combinaciones posibles de caras (o) y cruz (x).<br />
 Si definimos la variable aleatoria X como cantidad de caras entonces X(s) será la cantidad de caras del experimento.<br />
 Si s resulta ser la secuencia <code>ooxxxoxxxo</code> entonces X(s) = 4.</p>
</blockquote>
<h3 id="distribucion-de-una-variable-aleatoria"><a class="toclink" href="../../2019/04/03/funciones-de-probabilidad-y-distribucion/#distribucion-de-una-variable-aleatoria">Distribucion de una variable aleatoria</a></h3>
<p>Si tenemos la distribución de probabilidad del espacio muestral del experimento podemos determinar la distribución de probabilidad de cualquier variable aleatoria válida.</p>
<p>Volviendo al ejemplo de la moneda. Dijimos que hay 2^10^ combinaciones de cara o cruz.
La cantidad de combinaciones de X caras en 10 lanzamientos es <span class="arithmatex">\(P(X = x) = \binom{n}{x} \frac{1}{2^{10}}\)</span>  para <span class="arithmatex">\(x = 0,1,2,..,10\)</span></p>
<h3 id="distribuciones-discretas"><a class="toclink" href="../../2019/04/03/funciones-de-probabilidad-y-distribucion/#distribuciones-discretas">Distribuciones Discretas</a></h3>
<p>Una variable aleatoria tiene una distribución discreta si solo puede tomar valores de una secuencia (generalmente finita pero puede no serlo). </p>
<ul>
<li>La función de probabilidad le otorga una probabilidad puntual a cada valor de esa secuencia.</li>
<li>Los valores por fuera de la secuencia tienen probabilidad  = 0</li>
<li>La suma de todas las probabilidades tiene que ser 1</li>
</ul>
<h4 id="distribucion-uniforme"><a class="toclink" href="../../2019/04/03/funciones-de-probabilidad-y-distribucion/#distribucion-uniforme">Distribución Uniforme</a></h4>
<p>En el caso de la dsitribución uniforme, supongamos que la variable puede tomar valores de 1 a k.
La función de probabilidad será <span class="arithmatex">\(f(x) = \frac{1}{k}\)</span> para x = 1,2,...,k. 
Y 0 para todos los otros valores.</p>
<blockquote>
<p>si k = 10<br />
Los valores de la variable serán cualquier entero entre 1 y 10<br />
Cada valor tendrá probabilidad <span class="arithmatex">\(\frac{1}{10}\)</span></p>
</blockquote>
<h4 id="distribucion-binomial"><a class="toclink" href="../../2019/04/03/funciones-de-probabilidad-y-distribucion/#distribucion-binomial">Distribución Binomial</a></h4>
<p>En el caso de la dsitribución binomial se asumen dos posibles resultados, uno con probabilidad <em>p</em> y su contraparte con probabilidad <em>1-p</em>.<br />
Por ejemplo la probabilidad p de que una máquina genere un producto defectuoso y 1-p de que sea no defectuoso.<br />
Si una máquina produce <em>n</em> productos va a generar X productos defectuosos. La variable aleatoria X tendrá una distribución discreta y sus posibles valores irán de 0 a n.<br />
Para cualquier valor de x (entre 0 y n), la probabilidad de que la máquina genere x productos defectuosos entre los n producidos (de una secuencia particular) es <span class="arithmatex">\(p^{x}q^{(n-x)}\)</span><br />
Como existen <span class="arithmatex">\(\binom{n}{x}\)</span> distintas secuencias posibles con x defectuosos entre los n productos tenemos que:<br />
<span class="arithmatex">\(Pr(X = x) = \binom{n}{x}p^{x}q^{(n-x)}\)</span><br />
La función de probabilidad será <span class="arithmatex">\(f(x) = \binom{n}{x}p^{x}q^{(n-x)}\)</span> para x = 0,1,2,...,n. 
Y 0 para todos los otros valores.</p>
<p>Para usar esta distribución en R tenemos los siguientes comandos:</p>
<ul>
<li>Para generar <em>n</em> escenarios al azar donde se producen <em>size</em> productos con probabilidad <em>p</em> de ser defectuosos.
El resultado es la variable x por escenario. Es decir la cantidad de defectuosos.<br />
En el primer escenario x = 0, en el segundo x = 1 y así.</li>
</ul>
<pre><code class="language-r">set.seed(1)
rbinom(n = 10, size = 5, p = 0.2 )
</code></pre>
<pre><code>##  [1] 0 1 1 2 0 2 3 1 1 0
</code></pre>
<pre><code class="language-r"># random binomial
</code></pre>
<ul>
<li>Para saber la probabilidad de obtener <em>x</em> productos defectuosos si una máquina produce <em>size</em> productos y la probabilidad de que produzca un defectuoso es <em>prob</em>.<br />
Hay probabilidad de 0.0264 de obtener 5 defectuosos si producimos 10 con probabilidad 0.2.</li>
</ul>
<pre><code class="language-r">dbinom(x = 5, size = 10, prob = 0.2)
</code></pre>
<pre><code>## [1] 0.02642412
</code></pre>
<ul>
<li>Para saber la probabilidad acumulada de obtener <em>q</em> <strong>o menos</strong> productos defectuosos si la máquina fabrica <em>size</em> objetos, con probabilidad de defecto <em>prob</em>.
Hay probabiliad de 0.879 de obtener 3 o menos defectuosos si la máquina produce 10 objetos con probabilidad 0.2 de defecto.
Es decir, es la suma de obtener exactamanete 0 defectuosos, más exactamente 1 defectuoso, más exactamnente 2 defectuosos, más exactamente 3 defectuosos.</li>
</ul>
<pre><code class="language-r">pbinom(q = 3, size = 10, prob = 0.2)
</code></pre>
<pre><code>## [1] 0.8791261
</code></pre>
<h3 id="distribuciones-continuas"><a class="toclink" href="../../2019/04/03/funciones-de-probabilidad-y-distribucion/#distribuciones-continuas">Distribuciones Continuas</a></h3>
<p>Una variable aleatoria X tiene una distribución continua si existe una función <code>f</code> definida en los reales tal que para algún intervalo A<br />
<span class="arithmatex">\(Pr(X \in A) = \int_{A} f(x)\)</span>  </p>
<p>La función <code>f</code> es la <em>función de densidad de probabilidad</em>. PDF por sus siglas en inglés.<br />
La probabilidad de que X tome algún valor en un intervalo se encuentra integrando <code>f</code> en ese rango.</p>
<p>Por ejemplo para la distribución uniforme en un intervalo <em>(a,b)</em> podemos ver que su pdf (o función de densidad de probabilidad) es<br />
<span class="arithmatex">\(f(x) = \begin{cases}\frac{1}{b-a} &amp; \text{para } a \leq x \leq b \\ 0 &amp; \text{resto}\\  \end{cases}\)</span></p>
<h4 id="distribucion-normal"><a class="toclink" href="../../2019/04/03/funciones-de-probabilidad-y-distribucion/#distribucion-normal">Distribución Normal</a></h4>
<p>Para la distribución Normal tenemos los siguientes comandos:  </p>
<ul>
<li>Para obtener <em>n</em> variables aleatorias provenientes de una normal con media <em>mean</em> y desvío <em>sd</em></li>
</ul>
<pre><code class="language-r">set.seed(1)
rnorm(n = 5, mean = 10, sd = 2)
</code></pre>
<pre><code>## [1]  8.747092 10.367287  8.328743 13.190562 10.659016
</code></pre>
<ul>
<li>Para obtener el valor de la pdf de la normal para algún valor de X en particular.
Recuerden que no es una probabilidad, solo es el valor de la función. Las probabilidad se encuentra integrando la función en el intervalo deseado.<br />
Si graficáramos los valores de dnorm para el intervalo -3,3 obtendríamos la forma típica de la normal.</li>
</ul>
<pre><code class="language-r">dnorm(0.5, mean = 0, sd = 1)
</code></pre>
<pre><code>## [1] 0.3520653
</code></pre>
<ul>
<li>Para obtener la probabilidad acumulada hasta determinado punto. También conocido como <em>Función de Distribución</em> o <em>Función de Distribución Acumulada</em> <strong>C.D.F. por sus siglas en ingles</strong>
Por ejemplo, cual es la probabilidad de obtener un valor igual o menos a 1.5 si tomamos una muestra de una normal estándar </li>
</ul>
<p><span class="arithmatex">\(N \sim (0,1)\)</span></p>
<pre><code class="language-r">pnorm(q = 1.5, mean = 0, sd = 1)
</code></pre>
<pre><code>## [1] 0.9331928
</code></pre>
<p>Hay 93.31% de chances de obtener un valor inferior a 1.5 si tomamos una muestra al azar de una normal con media 0 y desvío 1.</p>
<ul>
<li>La inversa también se puede calcular facilmente en R. Que valor debe tomar la variable aleatoria normal si deseo tenes un 93.31% de chances de obtener un valor menor o igual a ese?</li>
</ul>
<pre><code class="language-r">qnorm(p = 0.9331, mean = 0, sd = 1)
</code></pre>
<pre><code>## [1] 1.499284
</code></pre>
<p>La diferencia respecto al código anterior es porque redondeamos la probabilidad.</p>

    <nav class="md-post__action">
      <a href="../../2019/04/03/funciones-de-probabilidad-y-distribucion/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2019-03-30 00:00:00">2019-03-30</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../../category/r/" class="md-meta__link">R</a></li>
        
        
          
          <li class="md-meta__item">
            
              6 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="introduccion-a-graficos-con-mapas"><a class="toclink" href="../../2019/03/30/introduccion-a-graficos-con-mapas/">Introduccion a graficos con mapas</a></h2>
<h3 id="data"><a class="toclink" href="../../2019/03/30/introduccion-a-graficos-con-mapas/#data">Data</a></h3>
<p>Vamos a ver un ejemplo sencillo para representar información visualmente sobre mapas. En este caso un pequeño dataset de incendios forestales en Argentina de 2012 a 2015. La idea es usar ggplot y mantener el enfoque de gráficos por capas.</p>
<p>Vamos a necesitar.</p>
<pre><code>&gt; tidyverse
&gt; rgdal
&gt; rgeos
</code></pre>
<p>Tendremos como input las provincias, departamento, cantidad de focos por incendio, area afectada y año de inicio y fin.
Cada observación es un incendio.</p>
<p>Para este ejemplo nos vamos a centrar en las provincias, los focos y su efecto sin importar la fecha.</p>
<p>Empezamos cargando la data.</p>
<pre><code class="language-r">library(tidyverse)
# Load Raw data
raw &lt;- read.csv(&quot;../../static/post/2019-03-30-introduccion-a-graficos-en-mapas/focosincendio.csv&quot;, sep = &quot;;&quot;)
raw &lt;- as.tibble(raw)
</code></pre>
<pre><code>## Warning: `as.tibble()` was deprecated in tibble 2.0.0.
## i Please use `as_tibble()` instead.
## i The signature and semantics have changed, see `?as_tibble`.
</code></pre>
<pre><code class="language-r">glimpse(raw)
</code></pre>
<pre><code>## Rows: 120
## Columns: 11
## $ pais_id         &lt;int&gt; 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,~
## $ pais            &lt;chr&gt; &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina~
## $ provincia_id    &lt;int&gt; 6, 14, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 30, 30, 30, 30, 30, 30, 30, 30, 30, ~
## $ provincia       &lt;chr&gt; &quot;Buenos Aires&quot;, &quot;Córdoba&quot;, &quot;Corrientes&quot;, &quot;Corrientes&quot;, &quot;Corrientes&quot;, &quot;Corrientes&quot;, &quot;Corrientes&quot;, &quot;Corrientes&quot;, &quot;Corrientes&quot;, &quot;C~
## $ departamento_id &lt;int&gt; 833, 14, 56, 28, 28, 28, 70, 84, 84, 84, 112, 112, 119, 119, 126, 147, 154, 154, 161, 168, 168, 168, 168, 8, 15, 15, 15, 15, 28~
## $ departamento    &lt;chr&gt; &quot;Tres Arroyos&quot;, &quot;Calamuchita&quot;, &quot;General Alvear&quot;, &quot;Concepción&quot;, &quot;Concepción&quot;, &quot;Concepción&quot;, &quot;Goya&quot;, &quot;Itizaingó&quot;, &quot;Ituzaingo&quot;, &quot;I~
## $ sup_afectada    &lt;dbl&gt; 2400.00, 50.00, 257.00, 130.00, 5.00, 146.00, 30.00, 294.30, 378.00, 158.00, 300.00, 450.00, 450.00, 15.00, 20.00, 141.00, 295.~
## $ uni_med_id      &lt;chr&gt; &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;ha&quot;, &quot;~
## $ cant_focos      &lt;int&gt; 1, 1, 1, 2, 1, 1, 1, 3, 1, 3, 1, 1, 5, 2, 2, 1, 3, 1, 1, 18, 2, 3, 2, 1, 0, 0, 1, 7, 0, 2, 1, 0, 0, 0, 5, 10, 10, 3, 1, 1, 1, 1~
## $ año_inicial     &lt;int&gt; 2014, 2015, 2012, 2012, 2013, 2015, 2012, 2012, 2013, 2014, 2012, 2013, 2012, 2013, 2015, 2012, 2012, 2014, 2012, 2012, 2013, 2~
## $ año_final       &lt;int&gt; 2014, 2015, 2012, 2012, 2013, 2015, 2012, 2012, 2013, 2014, 2012, 2013, 2012, 2013, 2015, 2012, 2012, 2014, 2012, 2012, 2013, 2~
</code></pre>
<p>Exploramos un poco el dataset. Lo que nos vas a interesar represetnar es la segunda parte del código. Variables agregadas a nivel provincia.</p>
<pre><code class="language-r"># Generate Summary to Explore
sum_year &lt;- raw %&gt;% group_by(año_inicial) %&gt;%
  summarise(focos = sum(cant_focos), sup = sum(sup_afectada, na.rm = TRUE)) %&gt;%
  mutate(sup_prom = sup/focos)

# Actual data to be plotted
sum_prov &lt;- raw %&gt;% group_by(provincia) %&gt;%
  summarise(focos = sum(cant_focos), sup = sum(sup_afectada, na.rm = TRUE)) %&gt;%
  mutate(sup_prom = sup/focos) %&gt;%
  arrange(desc(sup_prom)) %&gt;%
  mutate(provincia = as.character(provincia))

head(sum_prov)
</code></pre>
<pre><code>## # A tibble: 6 x 4
##   provincia    focos   sup sup_prom
##   &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1 Buenos Aires     1 2400    2400  
## 2 Corrientes      55 5440.     98.9
## 3 Misiones        10  679      67.9
## 4 Córdoba          1   50      50  
## 5 Entre Ríos      12  594      49.5
## 6 Jujuy           62 2721.     43.9
</code></pre>
<p>Tenemos por provincia la cantidad de focos y la superficie  afectada. Vamos a usar la superficie promedio por foco para visualizar la magnitud de cada foco de incendio.</p>
<p>Ahora llega lo importante. ¿Cómo representar esta data provincial en un mapa de Argentina?</p>
<h2 id="mapa"><a class="toclink" href="../../2019/03/30/introduccion-a-graficos-con-mapas/#mapa">Mapa</a></h2>
<p>Necesitamos un <strong>ShapeFile</strong> de Argentina, que basicamente es el tipo de archivo que se usa para representar mapas en gráficos. Contiene divisiones del país (provincias) con sus respectivas coordenadas y nombres.</p>
<p>Utilizaremos data descargada del siguiente link.
url &lt;- "http://biogeo.ucdavis.edu/data/diva/adm/ARG_adm.zip"</p>
<p>Lo descargué y deszipee en la computadora. Lo leemos con una librería particular <em>RGDAL</em>.</p>
<p>dsn contiene la ruta a la carpeta con los archivos del shapefile.
Layer apunta a al set de archivos que contiene la data que queremos. Generalmente hay otros sets con información no relevante al gráfico.</p>
<pre><code class="language-r">argentina &lt;- rgdal::readOGR(dsn = &quot;../../static/post/2019-03-30-introduccion-a-graficos-en-mapas/ARG_adm&quot;, layer = &quot;ARG_adm1&quot;, use_iconv=TRUE, encoding='UTF-8')
</code></pre>
<pre><code>## OGR data source with driver: ESRI Shapefile 
## Source: &quot;D:\DataScience\StatsBlog\blogStats\static\post\2019-03-30-introduccion-a-graficos-en-mapas\ARG_adm&quot;, layer: &quot;ARG_adm1&quot;
## with 24 features
## It has 9 fields
## Integer64 fields read as strings:  ID_0 ID_1
</code></pre>
<p>Es un archivo S4 por lo que se utiliza "@" para acceder a su contenido. Por ejemplo:</p>
<pre><code class="language-r">head(argentina@data)
</code></pre>
<pre><code>##   ID_0 ISO    NAME_0 ID_1                 NAME_1           TYPE_1        ENGTYPE_1 NL_NAME_1
## 0   12 ARG Argentina    1           Buenos Aires        Provincia         Province      &lt;NA&gt;
## 1   12 ARG Argentina    2                Córdoba        Provincia         Province      &lt;NA&gt;
## 2   12 ARG Argentina    3              Catamarca        Provincia         Province      &lt;NA&gt;
## 3   12 ARG Argentina    4                  Chaco        Provincia         Province      &lt;NA&gt;
## 4   12 ARG Argentina    5                 Chubut        Provincia         Province      &lt;NA&gt;
## 5   12 ARG Argentina    6 Ciudad de Buenos Aires Distrito Federal Federal District      &lt;NA&gt;
##                                                                               VARNAME_1
## 0                                                                   Baires|Buenos Ayres
## 1                                                                               Cordova
## 2                                                                                  &lt;NA&gt;
## 3                                                        El Chaco|Presidente Juan Peron
## 4                                                                                  &lt;NA&gt;
## 5 BUENOS AIRES D.F.|Capital Federal|Distretto Federale|Distrito Federal|Federal Capital
</code></pre>
<p>Este tipo de archivos tiene una estructura complicada y hay varias librerías útiles. Con el fin de mantenernos dentro del tidyverse usaremos el enfoque de ggplot por capaz para graficar.
Primero necesitamos llevar la información del shapefile a un dataframe.</p>
<pre><code class="language-r"># Transformo a dataframe.
argentina_df &lt;- broom::tidy(argentina)
</code></pre>
<pre><code>## Regions defined for each Polygons
</code></pre>
<pre><code class="language-r"># id es la provincia
head(argentina_df)
</code></pre>
<pre><code>## # A tibble: 6 x 7
##    long   lat order hole  piece group id   
##   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt;
## 1 -60.2 -33.3     1 FALSE 1     0.1   0    
## 2 -60.2 -33.3     2 FALSE 1     0.1   0    
## 3 -60.2 -33.3     3 FALSE 1     0.1   0    
## 4 -60.2 -33.3     4 FALSE 1     0.1   0    
## 5 -60.2 -33.3     5 FALSE 1     0.1   0    
## 6 -60.2 -33.3     6 FALSE 1     0.1   0
</code></pre>
<p>Ahora genero un diccionario de ids con su respectiva provincia para poder linkear mi mapa con la data de incendios.</p>
<pre><code class="language-r">ids &lt;- cbind.data.frame(provincia = as.character(argentina@data$NAME_1), id = as.character(rownames(argentina@data))) %&gt;%
  mutate(provincia = as.character(provincia), id = as.character(id))
</code></pre>
<p>El dataframe generado a partir del shapefile tiene las coordenadas que forman cada provincia y el id. Ahora lo que haremos es pegarle la data de incendios para poder utilizarla sobre el mapa.
Está generado el log de los focos porque en una prueba intenté usar la data transformada para suavizar outliers pero no se verá en esta versión.</p>
<pre><code class="language-r"># Agrego provincia por ID y data de incendios a la data del shapefile.
argentina_df2 &lt;- argentina_df %&gt;% left_join(ids, by = &quot;id&quot;) %&gt;%
  left_join(sum_prov, by = &quot;provincia&quot;) %&gt;%
  mutate(focos = ifelse(is.na(focos),0.5,focos),
         logfocos = log(focos), 
         logfocos2 = logfocos - min(logfocos))
</code></pre>
<p>Por ahora vamos a poder ser capaces de graficar un mapa de argentina con sus provincias delimitadas y pintarlas según la cantidad de focos de incendio por ejemplo. Pero además vamos a querer agregar alguna forma sobre cada provincia. Por ejemplo un punto de distinto tamaño según el area afectada por cada foco en promedio. Para eso necesitamos localizar el centroide de cada poligono, es decir, el centro de cada provincia.
Para ellos usamos la librería <strong>RGEOS</strong>
Luego a cada centro le agrego la data que voy a querer usar. En este caso la superficie promedio afectada por foco de cada provincia.</p>
<pre><code class="language-r"># Calculo el centro de cada poligono (provincias)
# para obtener el &quot;centro&quot; donde iran los puntos o nombres.
centros &lt;- rgeos::gCentroid(argentina, byid = TRUE) %&gt;%
  as.data.frame() %&gt;%
  mutate(id = rownames(.))

# Agrego data relevante para el ploteo (superficie promedio)
centros2 &lt;- centros %&gt;% left_join(ids, by = &quot;id&quot;) %&gt;%
  left_join(sum_prov, by = &quot;provincia&quot;) %&gt;%
  mutate(focos = ifelse(is.na(focos),0.5,focos),
         sup = ifelse(is.na(sup),0,sup),
         log_sup_prom = log(sup_prom),
         sup_prom_sin_outlier = ifelse(sup_prom &gt; 150, 150,sup_prom) ) # esto es para suavizar el outlier. Buscar otro enfoque.
</code></pre>
<h3 id="grafico"><a class="toclink" href="../../2019/03/30/introduccion-a-graficos-con-mapas/#grafico">Grafico!</a></h3>
<p>Ya tenemos todo. Tenemos el mapa, tenemos la cantidad de focos de incendio por provincia y tenemos el centro de cada provincia donde vamos a incluir un punto que muestra la intensidad de los incendios.
Simplemente graficamos sigueindo la lógica por capas de ggplot.
Las provincias sin puntos son aquellas que no tuvieron ningún incendio.</p>
<pre><code class="language-r">ggplot() +
  geom_polygon(data = argentina_df2, aes(x=long, y = lat, group = group,  fill = focos), color = &quot;white&quot;) + # mapa de argentina
  # coloreado segun cantidad de focos
  coord_fixed(0.8) + # tamañp del mapa
  scale_fill_gradient2(&quot;Cantidad de Focos de incendio&quot;, low = &quot;white&quot;, mid = &quot;lightgreen&quot;, high = &quot;darkred&quot;) + # escala de colores para focos
  geom_point(data = centros2, aes(x = x, y = y, size = sup_prom_sin_outlier)) + # puntos por provincia con superficie promedio
  scale_size(name = &quot;Superficie Promedio Afectada (ha)&quot;,range = c(1,5)) + # escala de los puntos
  guides(fill = guide_legend(order = 1), # Orden de los leyendas a la derecha.
         size = guide_legend(order = 2)) # Por algun motivo esto discretizo la leyenda de focos
</code></pre>
<p><img alt="Image" src="../../img/2019-03-30-introduccion-a-graficos-con-mapas-unnamed-chunk-10-1.png" /></p>

    <nav class="md-post__action">
      <a href="../../2019/03/30/introduccion-a-graficos-con-mapas/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
      
        
          



<nav class="md-pagination">
  
</nav>
        
      
    </div>
  </div>

          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../2020/" class="md-footer__link md-footer__link--prev" aria-label="Previous: 2020" rel="prev">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                2020
              </div>
            </div>
          </a>
        
        
          
          <a href="../2018/" class="md-footer__link md-footer__link--next" aria-label="Next: 2018" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                2018
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.dfff1995.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.dff1b7c8.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>