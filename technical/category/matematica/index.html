
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../algebra/">
      
      
        <link rel="next" href="../blog/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.2.7">
    
    
      
        <title>matematica - Franco Betteo</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.046329b4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.85d0ee34.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="youtube" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#matematica" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Franco Betteo" class="md-header__button md-logo" aria-label="Franco Betteo" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Franco Betteo
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              matematica
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="youtube" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../.." class="md-tabs__link">
          
  
    
  
  Home

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
    
  
  Technical posts

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="https://sportsjobs.online" class="md-tabs__link">
        
  
    
  
  Job Board

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../nba_salaries/" class="md-tabs__link">
          
  
    
  
  NBA salaries legacy model

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Franco Betteo" class="md-nav__button md-logo" aria-label="Franco Betteo" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Franco Betteo
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../.." class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Home
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
    
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Technical posts
  </span>
  

            </a>
            
              <label class="md-nav__link " for="__nav_2">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Technical posts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Archive
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../archive/2024/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../archive/2022/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2022
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../archive/2021/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2021
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../archive/2020/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2020
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../archive/2019/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2019
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../../archive/2018/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2018
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" checked>
        
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Categories
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Categories
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../estadistica/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    estadistica
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../machine-learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    machine learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../statistics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    statistics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../python/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../r/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    R
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../algebra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    algebra
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    matematica
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    matematica
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bias-variance-tradeoff" class="md-nav__link">
    Bias Variance Tradeoff
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#distintas-distancias" class="md-nav__link">
    Distintas Distancias
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#esencia-del-algebra-lineal" class="md-nav__link">
    Esencia del Algebra Lineal
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cadenas-de-markov" class="md-nav__link">
    Cadenas de Markov
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../blog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    blog
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="https://sportsjobs.online" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Job Board
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../nba_salaries/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    NBA salaries legacy model
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#bias-variance-tradeoff" class="md-nav__link">
    Bias Variance Tradeoff
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#distintas-distancias" class="md-nav__link">
    Distintas Distancias
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#esencia-del-algebra-lineal" class="md-nav__link">
    Esencia del Algebra Lineal
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cadenas-de-markov" class="md-nav__link">
    Cadenas de Markov
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content" data-md-component="content">
    <div class="md-content__inner">
      <header class="md-typeset">
        <h1 id="matematica">matematica</h1>
      </header>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2022-01-17 00:00:00">2022-01-17</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../estadistica/" class="md-meta__link">estadistica</a>, 
              <a href="./" class="md-meta__link">matematica</a></li>
        
        
          
          <li class="md-meta__item">
            
              2 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="bias-variance-tradeoff"><a class="toclink" href="../../2022/01/17/bias-variance-tradeoff.en-us/">Bias Variance Tradeoff</a></h2>
<p>Mean squared error (MSE) is a measure of how far our prediction is from the true values of the dependent variable. It's the expectation of the squared error.</p>
<p>The squared error being:<br />
<span class="arithmatex">\(<span class="arithmatex">\((Y - \hat \mu(x))^2\)</span>\)</span> 
where Y is the true value and $ \hat \mu(x)$ is the prediction for a given x.</p>
<p>We can decompose it into:<br />
$$
(Y - \hat \mu(x))^2 \
= (Y - \mu(x) + \mu(x) - \hat \mu(x)^2) \
= (Y - \mu(x))^2 + 2(Y - \mu(x))(\mu(x) - \hat \mu(x)) + (\mu(x) - \hat \mu(x))^2
$$</p>
<p>So, that's the squared error. The MSE is the expectation of that.  </p>
<p>The expectation is a linear operator so we can apply it independently to different terms of a summation.<br />
The expectation of the first term is the variance of the error intrinsic to the DGP.<br />
The second term goes to 0 because involves <span class="arithmatex">\(E(Y-\mu(x))\)</span> that is the expectation of the error and that's equal to 0.<br />
The third term reamins as it is since doesn't involve random variables.  </p>
<div class="arithmatex">\[MSE(\hat \mu(x)) = \sigma^2_x + (\mu(x) - \hat \mu(x))^2\]</div>
<p>This is our first bias-variance decomposition. The first term is the intrinsic difficulty of the problem to model, is the variance of the error and can not be reduced, it is what it is.<br />
The second term is how off our predictions are regarding the true expected value for that particular X.  </p>
<p>This would be fine if we wouldn't need to consider <span class="arithmatex">\(\hat \mu(x)\)</span> a random variable itself, since it is dependent on the specific dataset we are using. Given another dataset our estimation would be different despite using the same model methodology.<br />
What we actually want is the MSE of the method used <span class="arithmatex">\(\hat M\)</span> and not only the result of a particular realization.</p>
<p>$$MSE(\hat M_n(x)) = E[(Y - \hat M_n(X))^2 | X=x] \
= ... \
= \sigma^2_x + (\mu(x) -  E[\hat M_n(x)])^2 - V[\hat M_n(x)]
$$
This is our 2nd bias-variance decomposition.<br />
The first term is still the irreducible error.<br />
The second term is the bias of using <span class="arithmatex">\(\hat M_n\)</span> to approximate <span class="arithmatex">\(\mu(x)\)</span>. Is the approximation bias/error.<br />
The third term is the variance of the estimate of the regression function. If our estimates have high variance we can have large errors despite using an unbiased approximation.  </p>
<p>Flexible methods will be able to approximate <span class="arithmatex">\(\mu(x)\)</span> closely, however usually using more flexible methods involve increasing the variance of the estimate. That's the <strong>bias-variance tradeoff</strong>. We need to evaluate how to balance that, sometimes including some bias reduce much more the error by decreasing the variance.<br />
Usually larger N decreases the MSE since it decreases bias and variance error.</p>
<h5 id="reference"><a class="toclink" href="../../2022/01/17/bias-variance-tradeoff.en-us/#reference">Reference</a></h5>
<p>Based on 1.4.1 from Advanced data analysis from a elementary point of view.</p>

    <nav class="md-post__action">
      <a href="../../2022/01/17/bias-variance-tradeoff.en-us/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2019-11-11 00:00:00">2019-11-11</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../algebra/" class="md-meta__link">algebra</a>, 
              <a href="../estadistica/" class="md-meta__link">estadistica</a>, 
              <a href="./" class="md-meta__link">matematica</a></li>
        
        
          
          <li class="md-meta__item">
            
              7 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="distintas-distancias"><a class="toclink" href="../../2019/11/11/distintas-distancias/">Distintas Distancias</a></h2>
<pre><code class="language-r">library(tidyverse)
</code></pre>
<p>Si tenemos un espacio euclideo, es decir, una linea, un plano o un hiperplano, que son los espacios
típicos de la geometría clásica, podemos calcular la distancia entre dos puntos que se hayen en él.</p>
<p>Es decir, cuál es la distancia entre los puntos A (1,1) y B (1,0) en un plano?
Empecemos pensando en los casos donde todos los valores del vector son numéricos.</p>
<pre><code class="language-r">A = c(0,0,1,1)
B = c(0,0,1,0)
recta = c(1,1,1,0)
df = as.data.frame(matrix(data = c(A,B, recta),
                          nrow = 4, 
                          ncol = 4,
                          byrow = TRUE )) %&gt;%
  rename( x0 = V1,
          y0 = V2,
          x1 = V3,
          y1 = V4)


ggplot(data=df[1:2,], aes(x=x0, y=y0)) + 
  geom_segment(aes(xend=x1, yend=y1),
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + 
  geom_point( aes(x = x1, y = y1), 
              color = &quot;red&quot;, size = 2)
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-2-1.png" /></p>
<h4 id="distancia-euclideana"><a class="toclink" href="../../2019/11/11/distintas-distancias/#distancia-euclideana">Distancia Euclideana</a></h4>
<p>La métrica más habitual que se utiliza es la distancia euclideana, que consiste en la recta que une ambos puntos. </p>
<pre><code class="language-r">ggplot(data=df[1:2,], aes(x=x0, y=y0))+
  geom_segment(aes(xend=x1, yend=y1),
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + 
  geom_point(aes(x = x1, y = y1), 
               color = &quot;red&quot;, size = 2) + 
  geom_segment(data = df[3,], 
               aes(xend=x1, yend=y1),
               color = &quot;blue&quot;, 
               arrow = arrow(length = unit(0.3,&quot;cm&quot;)))
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-3-1.png" /></p>
<p>Esta distancia se calcula con:<br />
<span class="arithmatex">\(<span class="arithmatex">\(d(A,B) = d(B,A) = \sqrt{(A_1 - B_1)^2 + (A_2 - B_2)^2 + ... + (A_n - B_n)^2}  
= \sqrt{\sum_{i=1}^n (A_i - B_i)^2}\)</span>\)</span></p>
<p>Como se ve en la imagen, los puntos A y B pueden verse como vectores que inician en el origen (0,0). La distancia euclidea es a su vez la distancia entre sus puntas, que a su vez puede pensarse como un vector de desplazamiento (de A a B por ejemplo).</p>
<p>En este caso la distancia euclidea es:
<span class="arithmatex">\(<span class="arithmatex">\(d(A,B) = \sqrt{ (1-1)^2 + (1 - 0)^2} = 1\)</span>\)</span>
Y que es algo visible en el gráfico.</p>
<p>De manera más general, podemos definir toda una familia de distancias en el espacio euclideo.
<em>Las distancias de Minkowsky.</em></p>
<p>La distancia Minkowsky de orden p es:
<span class="arithmatex">\(<span class="arithmatex">\(d(A,B) = d(B,A) = \Bigg({\sum_{i=1}^n |A_i - B_i|^p}\Bigg)^{1/p}\)</span>\)</span></p>
<p>Vemos que si p = 2, entonces la distancia de Minkowsky no es otra que la distancia euclideana.</p>
<h4 id="distancia-de-manhattan"><a class="toclink" href="../../2019/11/11/distintas-distancias/#distancia-de-manhattan">Distancia de Manhattan</a></h4>
<p>Otro valor que suele tomarse para p es p = 1, y eso corresponde a la <em>distancia de Manhattan</em>.</p>
<p>Esta distancia se calcula con:<br />
<span class="arithmatex">\(<span class="arithmatex">\(d(A,B) = d(B,A) =  |A_1 - B_1| + |A_2 - B_2| + ... + |A_n - B_n|  
=\sum_{i=1}^n |A_i - B_i|\)</span>\)</span></p>
<p>Es básicamente la suma de las diferencias absolutas entre las distintas dimensiones de los vectores.</p>
<p>Luce asi.</p>
<pre><code class="language-r">A = c(0,0,3,3)
B = c(0,0,2,1)
recta = c(2,1,3,3)
manhattan1 = c(2,1,3,1)
manhattan2 = c(3,1,3,3)

df = as.data.frame(matrix(data = c(A,B, recta, manhattan1, manhattan2),
                          nrow = 5, 
                          ncol = 4,
                          byrow = TRUE )) %&gt;%
  rename( x0 = V1,
          y0 = V2,
          x1 = V3,
          y1 = V4)


ggplot(data=df[1:2,], aes(x=x0, y=y0)) + 
    geom_point( aes(x = x1, y = y1), 
              color = &quot;red&quot;, size = 2) + 
      geom_segment(data = df[3,], 
               aes(xend=x1, yend=y1, color = &quot;blue&quot;),
               #color = &quot;blue&quot;, 
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) +
      geom_segment(data = df[4,], 
               aes(xend=x1, yend=y1,  color = &quot;green&quot;,),
               #color = &quot;green&quot;, 
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + 
      geom_segment(data = df[5,], 
               aes(xend=x1, yend=y1),
               color = &quot;green&quot;, 
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) +
      scale_colour_manual(name = 'the colour', 
         values =c('blue'='blue','green'='green'),
         labels = c('Euclideana','Manhattan'))
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-4-1.png" /></p>
<p>Vemos como el valor abosluto imposibilita ir en dirección diagonal. Lo que se logra es medir la distancia como si hubiera una grilla como la del gráfico. Su nombre proviene de su utilización para medir distancias al interior de una ciudad (uno no puede cruzar las manzanas por el medio!).</p>
<p>Para saber cual conviene utilizar hay que pensar en el problema en cuestión. </p>
<ul>
<li>Ya sea medir distancias en ciudades o donde haya restricciones de ese tipo puede que Manhattan sea más apropiado.  </li>
<li>Por otra parte al no elevar al cuadrado le da menos pesos a las grandes distancias o mismo outliers por lo que puede ser otro motivo válido.  </li>
<li>Por último, algunos trabajos argumentan que es más adecuada en problema de alta dimensionalidad (o mismo valores menores a 1 en el exponente de la formula de Minkowsky)</li>
</ul>
<h4 id="similaridad-coseno"><a class="toclink" href="../../2019/11/11/distintas-distancias/#similaridad-coseno">Similaridad coseno</a></h4>
<p>La similaridad coseno se utiliza cuando se quiere ver la similitud "angular" entre dos observaciones y no la distancia en el plano. Es decir, vemos la dirección pero no la magnitud</p>
<pre><code class="language-r">A = c(0,0,1,1)
B = c(0,0,2,2)
C = c(0,0,5,0)

df = as.data.frame(matrix(data = c(A,B,C),
                          nrow = 3, 
                          ncol = 4,
                          byrow = TRUE )) %&gt;%
  rename( x0 = V1,
          y0 = V2,
          x1 = V3,
          y1 = V4)


ggplot(data=df[1:3,], aes(x=x0, y=y0 )) + 
  geom_segment(aes(xend=x1, yend=y1),
               arrow = arrow(length = unit(0.3,&quot;cm&quot;))) + 
  geom_point( aes(x = x1, y = y1), 
              color = &quot;red&quot;, size = 2) + 
  geom_text(aes(x=x1, y = y1, label = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;)),
            vjust = -0.5)
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-5-1.png" /></p>
<p>Si hicieramos la distancia euclideando entre A y B obtendriamos el valor de la distancia en el plano, sin embargo podemos ver que se encuentran sobre la misma recta y por lo tanto su dirección es la misma. La similaridad coseno mide el ángulo entre dos puntos. En este caso el ángulo entre A y B es 0, y por ende su similaridad coseno es 1. Ambas tendrían la misma similaridad con cualquier otro punto de la misma recta, por más alejado que esté.
Respecto a C, tanto A y B tiene comparten el ángulo por lo tanto la similaridad coseno entre A y C será la misma que entre B y C.</p>
<pre><code class="language-r">cosA = c(1,1)
cosB = c(2,2)
cosC = c(5,0)

# Similaridad coseno entre A y B
lsa::cosine(cosA, cosB)[[1]]
</code></pre>
<pre><code>## [1] 1
</code></pre>
<pre><code class="language-r"># Similaridad coseno entre A y C
lsa::cosine(cosA, cosC)[[1]]
</code></pre>
<pre><code>## [1] 0.7071068
</code></pre>
<pre><code class="language-r"># Similaridad coseno entre B y C
lsa::cosine(cosC, cosB)[[1]]
</code></pre>
<pre><code>## [1] 0.7071068
</code></pre>
<p>Hay que tener en cuenta el contexto de nuestro problema para decidir qué medida de distancia usar. Por ejemplo la similaridad coseno se usa de manera estándar en análisis de texto (text mining).</p>
<h4 id="distancia-de-mahalanobis"><a class="toclink" href="../../2019/11/11/distintas-distancias/#distancia-de-mahalanobis">Distancia de Mahalanobis</a></h4>
<p>La distancia de Mahalanobis tiene la particularidad que mide la distancia entre un punto (P) y una distribución de datos (D). Si tenemos una nube de puntos correspondiente a una distribución D, cuanto más cerca esté P del centro de masa (o "promedio") más cerca se encuetran P y D. Intuitivamente sirve para pensar si P puede pertenecer a D o no.<br />
Dado que la nube de puntos no tiene por qué ser una esfera (donde cada dirección tiene la misma cantidad de puntos), hay que tener en cuenta cómo se dispersan los puntos alrededor del centro de masa.</p>
<p>No es lo mismo, </p>
<pre><code class="language-r">esfera = as.data.frame(MASS::mvrnorm(1000, mu = c(3,3), 
                                     Sigma = matrix(c(1,0,0,1),
                                                    nrow = 2,
                                                    ncol = 2)))

ggplot(data = esfera, aes(x = V1, y = V2)) + 
  geom_point() + 
  geom_point(data = as.data.frame(matrix(c(6,2),ncol = 2)), 
             aes(x = V1, y = V2), color = &quot;red&quot;) + 
  geom_text(data = as.data.frame(matrix(c(6,2),ncol = 2)),
            aes(x = V1, y = V2,label = &quot;P&quot;),
            vjust = 1.5, color = &quot;blue&quot;) +
  labs(title = &quot;Distribución esférica&quot;)
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-7-1.png" /></p>
<p>que,</p>
<pre><code class="language-r">elipse = as.data.frame(MASS::mvrnorm(1000, mu = c(3,3), 
                                     Sigma = matrix(c(1,0.6,0.6,1),
                                                    nrow = 2,
                                                    ncol = 2)))



ggplot(data = elipse, aes(x = V1, y = V2)) + 
  geom_point() + 
  geom_point(data = as.data.frame(matrix(c(6,2),ncol = 2)), 
             aes(x = V1, y = V2), color = &quot;red&quot;) + 
  geom_text(data = as.data.frame(matrix(c(6,2),ncol = 2)),
            aes(x = V1, y = V2,label = &quot;P&quot;),
            vjust = 1.5, color = &quot;blue&quot;) +
  labs(title = &quot;Distribución Elíptica&quot;)
</code></pre>
<p><img alt="Image" src="../../img/2019-11-11-distintas-distancias-unnamed-chunk-8-1.png" /></p>
<p>Los centros de masa son los mismos y lo único que cambia es la matriz de variancias y covarianzas (o como se correlacionan las variables). La distancia de P al centro es la misma, pero está claro que en el caso esférico P se encuentra más cerca de la distribución que en el caso elíptico.</p>
<p>Mahalanobis tiene en cuenta este aspecto ya que involucra la matriz de varianzas y covarianzas.</p>
<p>La distancia entre el punto x y la distribución con vector de medias <span class="arithmatex">\(\vec{\mu}\)</span> y matriz de covarianzas S es:
$$ D_M(\vec{x}) = \sqrt{(\vec{x} - \vec{\mu})^TS^{-1}(\vec{x} - \vec{\mu})})$$</p>
<p>Tanto el vector <span class="arithmatex">\(\vec{x}\)</span> como la distribución pueden ser multivariadas (como se ve en los gráficos de arriba).</p>
<p>Tener en cuenta que si tenemos dos puntos provenientes de la misma distribución, podemos usar la distancia de Mahalanobis como una medida de disimilaridad:
$$ D_M(\vec{x},\vec{y}) = \sqrt{(\vec{x} - \vec{y})^TS^{-1}(\vec{x} - \vec{y})})$$
Veamos por ejemplo como queda la distancia de P a las distribuciones esféricas y elípticas graficadas.</p>
<pre><code class="language-r"># Caso Esférico

mahalanobis(x = c(6,2), 
            center = c(3,3), 
            cov = matrix(c(1,0,0,1),
                         nrow = 2,
                         ncol = 2))
</code></pre>
<pre><code>## [1] 10
</code></pre>
<pre><code class="language-r"># Caso Elíptico
mahalanobis(x = c(6,2), 
            center = c(3,3), 
            cov = matrix(c(1,0.6,0.6,1),
                         nrow = 2,
                         ncol = 2))
</code></pre>
<pre><code>## [1] 21.25
</code></pre>
<p>Queda claro que P es más cercano a la distribución esférica que a la elíptica.</p>

    <nav class="md-post__action">
      <a href="../../2019/11/11/distintas-distancias/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2019-04-20 00:00:00">2019-04-20</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="./" class="md-meta__link">matematica</a>, 
              <a href="../algebra/" class="md-meta__link">algebra</a></li>
        
        
          
          <li class="md-meta__item">
            
              7 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="esencia-del-algebra-lineal"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/">Esencia del Algebra Lineal</a></h2>
<p>El álgebra lineal está por todas partes en estadística y data science. Matrices, vectores y transformaciones son términos que se escuchan seguido y están detrás de muchos de los métodos y algoritmos que se usan hoy por hoy. Aunque no sea necesario saber del tema para correr un modelo empaquetado en una librería de R, es muy útil entender lo que hacemos realmente ya que nos permite ver a los modelos como algo lógico y no una caja negra mágica.</p>
<p>Hay una serie de videos excelente en inglés que mediante visualizaciones y animaciones permite entender la intuición de muchos de los conceptos básicos, que solo con un libro puede ser medio críptico o poco imaginable.
Para el que le interese: <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">ESSENCE OF LINEAR ALGEBRA</a> por 3Blue1Brown.</p>
<p>Este post, aunque quizás medio desordenado y sin mucha prolijidad, es una recopilación de algunas notas. Puede que queden términos en inglés intercalados.</p>
<h3 id="matrices-y-vectores"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#matrices-y-vectores">Matrices y vectores</a></h3>
<ul>
<li>Vector vive en <em>n</em> dimensiones.</li>
<li>Suma de vectores es combinación lineal</li>
<li>En <span class="arithmatex">\(R^{2}\)</span> <span class="arithmatex">\(\hat \imath = \left&lt; 1, 0 \right&gt; \text{y} \hat \jmath = \left&lt; 0, 1 \right&gt;\)</span> forman una base. Cualquier punto es una combinación lineal de ellos.     </li>
<li>Span es el espacio que pueden generar x vectores. <span class="arithmatex">\(R^{2}\)</span> es el span de  <span class="arithmatex">\(\left&lt; 1, 0 \right&gt; \left&lt; 0, 1 \right&gt;\)</span></li>
<li>Vector puede ser pensado como una flecha desde el origen (0,0) a las coordenadas que lo identifican. O como un punto directo en las coordenadas..</li>
<li>Matriz es una transformación. Lleva un vector a otro punto. Si transformamos cada posible vector de un espacio por la matriz podemos ver como el espacio es transformado. Ej: rotar, invertir, estirar.</li>
<li>Si transformamos una base, cada punto nuevo puede generarse transformando la nueva base. <br />
Por ej: <span class="arithmatex">\(z = \left&lt; 3, 2 \right&gt; \text{es } 3\begin{bmatrix} 1 \\ 0 \end{bmatrix} + 2\begin{bmatrix} 0 \\ 1 \end{bmatrix} = 3\hat \imath + 2\hat \jmath\)</span><br />
Aplicando la transformación de la matriz A = <span class="arithmatex">\(\begin{pmatrix} A &amp; B \\ C &amp; D \end{pmatrix} \text{obtenemos los nuevos vectores base } \hat \imath^{*} \text{y } \hat \jmath^{*}\)</span><br />
<span class="arithmatex">\(z^{*} = 3\hat \imath^{*} + 2\hat \jmath^{*}\)</span> </li>
<li>Multiplicar 2 matrices es transformar un espacio con la primera matriz ( desde la derecha) y luego transformar el resultado por la segunda matriz. Ej: Rotar un espacio y luego invertirlo.</li>
<li>AB != BA -&gt; El orden de las transformaciones importa y se lee de derecha a izquierda.</li>
<li>
<p>La matriz (transformación) ya dice como van a ser las nuevas bases.<br />
Si la matriz es <span class="arithmatex">\(\begin{pmatrix} A &amp; B \\ C &amp; D \end{pmatrix}\)</span>, el nuevo <span class="arithmatex">\(\hat \imath^{*}\)</span> es <span class="arithmatex">\(\begin{bmatrix} A \\ C \end{bmatrix}\)</span> y <span class="arithmatex">\(\hat \jmath^{*}\)</span> es <span class="arithmatex">\(\begin{bmatrix} B \\ D \end{bmatrix}\)</span><br />
Ej: <span class="arithmatex">\(z = \left&lt; 3, 2 \right&gt;  z^{*} =  \begin{bmatrix} A &amp; B \\ C &amp; D \end{bmatrix}\begin{bmatrix} 3 \\ 2 \end{bmatrix} = \begin{bmatrix} 3A + 2B \\ 3C + 2D \end{bmatrix}\)</span><br />
Se puede ver también como:  <span class="arithmatex">\(<span class="arithmatex">\(z = 3\hat \imath + 2\hat \jmath \text{  }  z^{*} = 3\hat \imath^{*} + 2\hat \jmath^{*} = 3\begin{bmatrix} A \\ C \end{bmatrix} + 2\begin{bmatrix} B \\ D \end{bmatrix} = \begin{bmatrix} 3A + 2B \\ 3C + 2D \end{bmatrix}\)</span>\)</span>
&nbsp;</p>
</li>
<li>
<p>!!!. Las transformaciones afectan el area (en R2, el volumen en R3..) de las figuras en el espacio (todas por igual). El <em>DETERMINANTE</em> de una matriz mide ese cambio.</p>
</li>
<li>Si el determinante <strong>es 0</strong> significa que se perdió una dimensión o que todo se comprimió. Pasa de <span class="arithmatex">\(R^{2}\)</span> a una recta (o a un punto!)</li>
<li>Si el determinante <strong> es &lt; 0</strong> significa que el espacio se invirtió (en sentido.. como dar vuelta una hoja) pero |DET| siguen siendo el cambio en el area.</li>
<li>A^-1^A = I -&gt; una transformación que no hace nada.</li>
<li>
<p>Si DET(A) = 0 no existe la matriz inversa. Ej. <span class="arithmatex">\(R^{2}\)</span> -&gt; si det(A) = 0 la transformación lleva el espacio a una recta. No hay función que lleve cada vector de la recta a un punto en <span class="arithmatex">\(R{2}\)</span>. No hay vuelta atrás.
&nbsp;
&nbsp;</p>
</li>
<li>
<p>Si una transformación lleva todos los puntos a una recta tiene rango 1, si lleva a un plano rango 2, y así.. <strong>RANGO</strong> es el número de dimensiones del output. Rango completo es cuando mantiene las dimensiones del input.</p>
</li>
<li>El conjunto de posibles outputs de <span class="arithmatex">\(A\vec v\)</span> es el Column Space = Span de las columnas</li>
<li>Cuando perdés dimensiones por la transformación todo un conjunto de vectores pasa a ser (0,0). Eso se llama <strong>Null Space</strong> o <strong>Kernel</strong></li>
<li>Matrices no cuadradas cambian la dimensión del espacio.<br />
$$ \begin{bmatrix} A &amp; D \ B &amp; E \ C &amp; F \end{bmatrix} \begin{bmatrix} 1 \ 1 \end{bmatrix} = \begin{bmatrix} A + D \ B + E \ C + F \end{bmatrix} $$ 
Quedan todos los puntos de <span class="arithmatex">\(R^{2}\)</span> en un plano en el espacio <span class="arithmatex">\(R^{3}\)</span>. De acá viene la restricción para multiplicar matrices. La cantidad de columnas de la transformación tiene que ser igual a la dimensión del input</li>
</ul>
<h4 id="dot-product-o-producto-interno"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#dot-product-o-producto-interno">DOT PRODUCT o PRODUCTO INTERNO</a></h4>
<ul>
<li>Dot product entre dos vectores equivale a proyectar uno en el otro y multiplicar sus largos. <span class="arithmatex">\(\vec A \cdot \vec B = |A^{*}| * |B|\)</span><br />
<span class="arithmatex">\(A^{*}\)</span> es el vector A proyectado en B.</li>
<li><span class="arithmatex">\(\vec B\)</span> es un vector 2D pero también se lo puede ver como una matriz 1x2 que lleva del 2D a la recta.<br />
<span class="arithmatex">\(\vec B \cdot \vec A = B \vec A \text{que sería llevar A al espacio transformado por B.}\)</span><br />
<span class="arithmatex">\(B = \begin{bmatrix} B_x &amp; B_y \end{bmatrix}\)</span> tiene en sus columnas donde queda <span class="arithmatex">\(\hat \imath \text{y } \hat \jmath\)</span> (los vectores unitarios) al ser transformados o algun valor escalado de esto.<br />
<span class="arithmatex">\(\vec A \cdot \vec B\)</span> es el valor de A en la recta a la que te lleva la transformación B.</li>
<li>Es equivalente proyecto B en A.</li>
<li>Si Dot Product &gt; 0, tienen dirección similar.</li>
<li>Si Dot Product = 0, son ortogonales - proyección que cae en el origen.</li>
<li>Si Dot Product &lt; 0, tienen direcciones opuestas.</li>
</ul>
<p>&nbsp;
&nbsp;</p>
<h4 id="cross-product"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#cross-product">CROSS PRODUCT</a></h4>
<ul>
<li>Está definido para vectores en <span class="arithmatex">\(R^{3}\)</span></li>
<li>El cross product <span class="arithmatex">\(\vec u \times \vec v\)</span> es el area del paralelograma que se puede imaginar con las paralelas de los vectores (imaginandolo en <span class="arithmatex">\(R^{2}\)</span>. El signo depende de la orientación de los vectores. El vector de la "derecha" tiene que estar primero para que el cross product sea &gt; 0.</li>
<li>En realidad el paralelograma formado por dos vectores en R^3^ tiene area equivalente al <strong>Largo</strong> del vector output de su cross product. Ese vector es ortogonal al paralelograma.</li>
</ul>
<p>&nbsp;
&nbsp;</p>
<h4 id="cambio-de-base"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#cambio-de-base">CAMBIO DE BASE</a></h4>
<ul>
<li>Distintos sistemas de coordenadas definen <span class="arithmatex">\(\hat \imath = \left&lt; 1, 0 \right&gt;, \hat \jmath \left&lt; 0, 1 \right&gt;\)</span> como algo distinto. <strong>NO</strong> hay una sola "grilla" válida. El espacio no tiene grilla predeterminada.</li>
<li>Un mismo vector tiene distintas coordenadas según el sistema de bases desde donde se lo mire.</li>
<li>Para pasar de una base a otra se aplica una transformación lineal.<br />
Si <span class="arithmatex">\(\vec v\)</span> es un vector que queremos pasar de una base a otra, lo transformamos por la nueva base.<br />
Y <span class="arithmatex">\(\hat \imath^{*} =  \left&lt; \hat \imath^{*}_1, \hat \imath^{*}_2 \right&gt;, \hat \jmath^{*} =  \left&lt; \hat \jmath^{*}_1, \hat \jmath^{*}_2 \right&gt;\)</span>
Entonces:
<span class="arithmatex">\(<span class="arithmatex">\(\begin{bmatrix} \hat \imath^{*}_1 &amp; \hat \jmath^{*}_1 \\ \hat \imath^{*}_2 &amp; \hat \jmath^{*}_2 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \end{bmatrix} = \begin{bmatrix} v^{*}_1 \\ v^{*}_2 \end{bmatrix}\)</span>\)</span><br />
Donde <span class="arithmatex">\(\begin{bmatrix} v^{*}_1 \\ v^{*}_2 \end{bmatrix}\)</span> es el vector en la nueva base, es decir, serían las coordenadas del vector <span class="arithmatex">\(\vec v\)</span> en el nuevo sistema de coordenadas y representando ese punto bajo el sistema de coordenadas original. -&gt; Como se vería <span class="arithmatex">\(\vec v\)</span> en la nueva base? desde un punto de vista de la base original.<br />
La matriz transforma un vector siguiendo en el lenguaje de la base original.<br />
Ej: Si <span class="arithmatex">\(\vec v\)</span> es (1,2) en el sistema cartesiano típico y aplicamos la matriz de cambio de base, un vector (1,2) bajo otros ejes se ubicaría en otro punto del espacio. Qué punto es ese bajo el sistema cartesiano? Es (1,2) en el nuevo, pero queremos saber su equivalente en el sistema original.</li>
<li>Por otra parte si queremos saber que coordenadas tomaría el vector <span class="arithmatex">\(\vec v\)</span> bajo otra base debemos multiplicar por la inversa de la transformación. Transforma el vector al lenguaje de la nueva base.
Responde a la pregunta. Qué coordenadas toma el punto V_1, V_2 del espacio en el sistema nuevo?
&nbsp;</li>
<li>Para aplicar una transformación a otra base conviene llevar el vector a transformar a la base original, transformar y reconvertir a la nueva base.
$$ [A]^{-1}[T][A]\vec v = \vec v^{*}$$<br />
A lo expresa en términos de la base original, luego se le aplica la transformación T y luego se lo devuelve al lenguaje de la nueva transformación.</li>
</ul>
<p>&nbsp;
&nbsp;</p>
<h4 id="eigenvalues-y-eigenvectors-autovalores-y-autovectores"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#eigenvalues-y-eigenvectors-autovalores-y-autovectores">Eigenvalues y Eigenvectors (autovalores y autovectores)</a></h4>
<ul>
<li>!!! Al aplicar una transformación lineal a un espacio algunos vectores no cambian de dirección, solo se estiran o contraen pero sobre la misma recta. El resto sí se mueve. Los que se mantienen son los eigenvectors, y su factor de expansión o contracción es su eigenvalue.<br />
Si A es la matriz de transformación, <span class="arithmatex">\(\vec v\)</span> es un eigenvector y <span class="arithmatex">\(\lambda\)</span> su eigenvalue.
$$ A\vec v = \lambda \vec v$$  </li>
<li>Si una transformación es una matriz diagonal, lo único que hace es estirar <span class="arithmatex">\(\hat \imath \text{y } \hat \jmath\)</span> por lo tanto los vectores base son eigenvectors y la diagonal son los eigenvalues.</li>
<li>Si cambiamos la base a una formada por los eigenvectors (que spanean el espacio) de la matriz podemos aplicar la transformación (la matriz original de donde salieron los eigenvectors) a esta nueva base y solo la va a estirar, por lo tanto es una transformación diagonal, que permite calculos mucho más fácil. Después habría que volver a la base original.<br />
A -&gt; Matriz de transformación
E -&gt; Matriz de autovectores que forman la nueva base <span class="arithmatex">\(\begin{bmatrix} e_11 &amp; e_21 \\ e_12 &amp; e_22 \end{bmatrix}\)</span>
D -&gt; Matriz Diagonal cuyos valores son los eigenvalues
$$ E^{-1}AE=D$$</li>
</ul>
<p>E cambia la base a eigenvectors (expresado en la base original), A aplica transformación y E^-1^ lo lleva al lenguaje de la nueva base (queda expresado en las nuevas coordenadas)
&nbsp;
&nbsp;</p>
<h4 id="espacios-vectoriales-abstractos"><a class="toclink" href="../../2019/04/20/esencia-del-algebra-lineal/#espacios-vectoriales-abstractos">Espacios Vectoriales Abstractos</a></h4>
<ul>
<li>!!! Ver <em>funciones</em> como un tipo especial de vectores.</li>
<li>Las funciones se pueden sumar y escalar <span class="arithmatex">\(f(x) + g(x) \text{y } 2f(x)\)</span></li>
<li>Existen transformaciones lineales de funciones, convierten una función en otra. También conocidas como "operadores"</li>
<li>Para que una transformación sea lineal tiene que cumplir aditividad y mulitplicación por escalar<br />
$$ L(\vec v + \vec  w) = L(\vec v) + L(\vec w)$$
$$ L(c\vec v) = cL(\vec v)$$</li>
<li>En general cualquier espacio que cumpla los axiomas los espacios vectoriales puede ser considerado uno.</li>
</ul>

    <nav class="md-post__action">
      <a href="../../2019/04/20/esencia-del-algebra-lineal/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2018-08-25 00:00:00">2018-08-25</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="./" class="md-meta__link">matematica</a></li>
        
        
          
          <li class="md-meta__item">
            
              4 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="cadenas-de-markov"><a class="toclink" href="../../2018/08/25/cadenas-de-markov/">Cadenas de Markov</a></h2>
<p>Las cadenas de Markov son herramientas muy útiles para modelar transiciones entre estados. Imaginemos un escenario sencillo con dos posibles estados: día de lluvia o día soleado. En el momento <strong>t</strong> el estado supongamos que es "día soleado". ¿Cuál será el estado en <strong>t+1</strong> ? Dadas las probabilidades de transición de un estado a otro podemos simular escenarios tras el paso del tiempo. Mismo en algunos casos puede ser útil entender si en el largo plazo esta simulación converge hacia algún resultado estable en el tiempo.</p>
<p>Mostraremos ejemplos sencillos, pero la metodología es escalable a procesos complejos como pueden usarse en meteorología, aplicaciones financieras, etc. El <a href="http://blog.kleinproject.org/?p=1605&amp;lang=es">algoritmo de búsqueda de Google</a> está basado en cadenas de Markov, para que vean la utilidad. [^1]</p>
<p>Lo que necesitamos para este ejercicio son las probabilidades de transición de un estado a otro y los valores iniciales en cada estado.</p>
<p>Supongamos una clase de estadística con n estudiantes, en la que dos estados son posibles. 
Estado A : El alumno está atento.
Estado B : El alumno está aburrido.</p>
<ul>
<li>Las probablidades son las siguientes: <ul>
<li>En t+1 el 80% de A sigue en A (y por lo tanto el 20% de A pasa a B)</li>
<li>En t+1 el 25% de B pasa a A (y por lo tanto el 75% de B queda en B)</li>
</ul>
</li>
</ul>
<p>La matriz que representa esa probabilidades la vamos a llamar * Matriz de Transición *.</p>
<pre><code class="language-r">tmatrix &lt;-  as.matrix(c(0.8,0.25))  
# 80% de A sigue en A, 25% de B pasa a A en t+1

tmatrix &lt;- t(tmatrix) 
# trasponemos porque necesitamos esta información en la primera fila.

tmatrix &lt;- rbind(tmatrix, 1 - tmatrix[1,]) 
# Agregamos la segunda fila, que es 1 menos la primera.

tmatrix # Matriz de transción
</code></pre>
<pre><code>##      [,1] [,2]
## [1,]  0.8 0.25
## [2,]  0.2 0.75
</code></pre>
<p>La matriz de valores iniciales es la siguiente:
En un primero momento (t), el 10% de los alumnos está atento y el 90% aburrido.</p>
<pre><code class="language-r">smatrix &lt;- as.matrix(c(0.1,0.9)) # Matriz inicial, 10% Atento, 90% aburridos
</code></pre>
<p>Ya tenemos toda la información necesaria para hacer nuestras primeras simulaciones.
Supongamos que cada período de tiempo son 10 minutos, por lo tanto si t es el momento 0, t+1 es a los 10 de minutos de empezada la clase, t+2 a los 20 y así sucesivamente.</p>
<p>Para evaluar el porcentaje de alumnos concentrados en determinado momento de la clase lo que debemos hacer es multiplicar la matriz de estado inicial por la matriz de transición tantas veces como momentos a futuro querramos simular.</p>
<p>Por ejemplo si queremos ver que pasará con nuestros alumnos luego de 20 minutos de clase debemos multiplicar smatrix * tmatrix 2 veces.</p>
<pre><code class="language-r">for (i in 1:2){            # Loopeamos 2 veces. %*% es la multiplicacion matricial.
  smatrix = tmatrix %*% smatrix  
# smatrix va tomando nuevos valores en cada iteracion, 
# reflejando el movimiento de un estado a otro
}

smatrix  # Despues de 2 iteraciones -&gt; A 42% , B 58%
</code></pre>
<pre><code>##         [,1]
## [1,] 0.41775
## [2,] 0.58225
</code></pre>
<p>Vemos que luego de 2 transiciones el estado A está compuesto por casi 42% de alumnos (concentrados) y 58% no atentos. Son movimientos bastante rápidos de un estado a otro.</p>
<p>Veamos qué pasa luego de 10 iteraciones.</p>
<pre><code class="language-r">smatrix &lt;- as.matrix(c(0.1,0.9)) # Reseteamos smatrix a su estado inicial

for (i in 1:10){           
# Loopeamos 10 veces. %*% es la multiplicacion matricial.
  smatrix = tmatrix %*% smatrix   
# smatrix va tomando nuevos valores en cada iteracion, 
# reflejando el movimiento de un estado a otro
}

smatrix  # Despues de 10 iteraciones -&gt; A 55% , B 45%
</code></pre>
<pre><code>##           [,1]
## [1,] 0.5544017
## [2,] 0.4455983
</code></pre>
<p>Luego de 10 movimientos, A pasa a tener el 55% de los alumnos dejando a 45% en B (no atentos). En este ejemplo con el paso del tiempo los alumnos se concentran más y más en la clase. Pero es este un proceso continuo e infinito? Llega un momento en que dada la matriz de transción todos los alumnos pasan a estar en el estado A, es decir, atentos?</p>
<p>Para analizar esto podemos ver qué sucede luego de 1000 iteraciones (sería una clase muy muy larga...)</p>
<pre><code class="language-r">smatrix &lt;- as.matrix(c(0.1,0.9)) # Reseteamos smatrix a su estado inicial

for (i in 1:1000){  # Loopeamos 10 veces. %*% es la multiplicacion matricial.
  smatrix = tmatrix %*% smatrix  
# smatrix va tomando nuevos valores en cada iteracion,
# reflejando el movimiento de un estado a otro
}

smatrix  # Despues de 1000 iteraciones -&gt; A 55% , B 45%
</code></pre>
<pre><code>##           [,1]
## [1,] 0.5555556
## [2,] 0.4444444
</code></pre>
<p>El resultado es casi idéntico luego de 1000 iteraciones al intento de tan solo 10 iteraciones. Por lo tanto podemos ver que esta cadena de Markov converge rápidamente a 55% en A y 45% en B. Es un estado estacionario del cual no podemos movernos dada la matriz de transición que tenemos.</p>
<p>Este ejemplo sencillo permite ver la intuición detrás de esta potente herramienta en unos pocos pasos. Obviamente como comentamos antes se puede utilizar para procesos mucho más complejos y de muy diversas areas. 
Queríamos mostrar los fundamentos con una aplicación rápida y que se comprenda que lo que se requiere es una matriz de transición y un estado inicial.</p>
<p>[^1]: Para una explicación gráfica y muy didáctica dejamos el siguiente <a href="http://setosa.io/ev/markov-chains/">link en inglés.</a></p>

    <nav class="md-post__action">
      <a href="../../2018/08/25/cadenas-de-markov/">
        Continue reading
      </a>
    </nav>
  </div>
</article>
      
      
        
          



<nav class="md-pagination">
  
</nav>
        
      
    </div>
  </div>

          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../algebra/" class="md-footer__link md-footer__link--prev" aria-label="Previous: algebra" rel="prev">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                algebra
              </div>
            </div>
          </a>
        
        
          
          <a href="../blog/" class="md-footer__link md-footer__link--next" aria-label="Next: blog" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                blog
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["announce.dismiss", "content.action.edit", "content.action.view", "content.code.annotate", "content.code.copy", "content.code.select", "content.tabs.link", "content.tooltips", "header.autohide", "navigation.expand", "navigation.footer", "navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.prune", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.dfff1995.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.dff1b7c8.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>